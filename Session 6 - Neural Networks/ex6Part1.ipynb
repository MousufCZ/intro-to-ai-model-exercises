{"cells":[{"cell_type":"markdown","metadata":{"id":"ziOgNKQhRVUI"},"source":["# Tutorial 6 (Introduction to AI)\n","\n","# Neural Networks: MLP (Part 1)"]},{"cell_type":"markdown","metadata":{"id":"nrSV2X3_RVUN"},"source":["## 1. Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"N4J-RcluRVUO"},"source":["**Neural networks** require their implementor to meet a number of conditions. These conditions include:\n","\n","- an adequately sized dataset to both train and test the network.\n","- an understanding of the basic nature of the problem to be solved so that basic first-cut decision on creating the network can be made. These decisions include the activation and the learning methods.\n","- an understanding of the development tools.\n","- adequate processing power (some applications demand real-time processing that exceeds what is available in the standard, sequential processing hardware. The development of hardware is the key to the future of neural networks).\n","\n","Once these conditions are met, neural networks offer the opportunity of solving problems in areas where other approaches might lack either the processing power or a step-by-step methodology to come to a solution."]},{"cell_type":"markdown","metadata":{"id":"bXtfYrV6RVUP"},"source":["## Introduction to Keras and TensorFlow"]},{"cell_type":"markdown","metadata":{"id":"eVoBuPNKRVUP"},"source":["## Keras\n","\n","[Keras](https://keras.io/api/) is a high level tool for describing neural networks, which comes alongside Tensorflow.  Keras sits as a layer on top of Tensorflow, making it much easier to create neural networks.  Rather than define the graphs, you define the individual layers of the network with a much more high level API.  Unless you are performing research into entirely new structures of deep neural networks it is unlikely that you need to program TensorFlow directly.  "]},{"cell_type":"markdown","metadata":{"id":"cj4h_2cZRVUP"},"source":["## TensorFlow\n","\n","TensorFlow is a powerful open-source software library for AI and machine learning developed by  Google. TensorFlow allows distribution of computation across different computers, as well as multiple CPUs and GPUs within a single machine. Here our examples should run on a single core. TensorFlow provides a Python API (as well as a less documented C++ API)."]},{"cell_type":"markdown","metadata":{"id":"K3dBgbmNRVUQ"},"source":["# Classification or Regression\n","\n","Like many models, neural networks can function in classification or regression:\n","\n","* **Regression** - You expect a number as your neural network's prediction.\n","* **Classification** - You expect a class/category as your neural network's prediction.\n","\n","The following shows a classification and regression neural network:\n","\n","![class_2_ann_class_reg.png](attachment:class_2_ann_class_reg.png)\n","\n","Notice that the output of the regression neural network is numeric and the output of the classification is a class.  Regression, networks always have a single output.  Classification neural networks have an output neuron for each class."]},{"cell_type":"markdown","metadata":{"id":"XxWgc3dwRVUR"},"source":["First, let's check TensorFlow:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"svvPqh0mRVUR","outputId":"96627a4b-a147-485a-d7fc-879a1fcc515f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor Flow Version: 2.6.0\n","Keras Version: 2.6.0\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","print(\"Tensor Flow Version: {}\".format(tf.__version__))\n","\n","print(f\"Keras Version: {keras.__version__}\")"]},{"cell_type":"markdown","metadata":{"id":"g82pzLtWRVUU"},"source":["## 1. Neural Network regression using MPG\n","\n","This week we will return to some familiar examples: the auto-mpg dataset and the iris dataset, and use them to illustrate neural networks using Keras.\n","\n","This example shows how to encode the auto-mpg dataset for regression.  Remember that:\n","\n","* Input has both numeric and categorical features\n","* Input has missing values\n","\n","This example uses some of the functions defined below, the \"helpful functions\".  You've seen many of these helper functions before and they allow you to work with your data for better application of AI algorithms, such as neural networks. Consider the following:\n","\n","* Predictors/Inputs\n","    * Fill any missing inputs with the median for that column.  Use **missing_median**.\n","    * Encode textual/categorical values with **encode_text_dummy**.\n","    * Encode numeric values with **encode_numeric_zscore** (we'll also see scaling of the data).\n","* Output\n","    * Discard rows with missing outputs.\n","    * Encode textual/categorical values with **encode_text_index** (which in turn uses sklearn's LabelEncoder). Or use keras to_categorical method.\n","    * Do not encode output numeric values.\n","* Produce final feature vectors (X) and expected output (y) with **to_xy**.\n","\n","To encode categorical values that are part of the feature vector, use the functions from below. If the categorical value is the target (as was the case with Iris) use the same technique as Iris. The iris technique allows you to decode back to Iris text strings from the predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCBaMChBRVUV"},"outputs":[],"source":["import base64\n","import os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import requests\n","from sklearn import preprocessing\n","\n","# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n","def encode_text_dummy(df, name):\n","    dummies = pd.get_dummies(df[name])\n","    for x in dummies.columns:\n","        dummy_name = f\"{name}-{x}\"\n","        df[dummy_name] = dummies[x]\n","    df.drop(name, axis=1, inplace=True)\n","\n","# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n","# at every location where the original column (name) matches each of the target_values.  One column is added for\n","# each target value.\n","def encode_text_single_dummy(df, name, target_values):\n","    for tv in target_values:\n","        l = list(df[name].astype(str))\n","        l = [1 if str(x) == str(tv) else 0 for x in l]\n","        name2 = f\"{name}-{tv}\"\n","        df[name2] = l\n","\n","# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n","def encode_text_index(df, name):\n","    le = preprocessing.LabelEncoder()\n","    df[name] = le.fit_transform(df[name])\n","    return le.classes_\n","\n","# Encode a numeric column as zscores\n","def encode_numeric_zscore(df, name, mean=None, sd=None):\n","    if mean is None:\n","        mean = df[name].mean()\n","    if sd is None:\n","        sd = df[name].std()\n","    df[name] = (df[name] - mean) / sd\n","\n","# Convert all missing values in the specified column to the median\n","def missing_median(df, name):\n","    med = df[name].median()\n","    df[name] = df[name].fillna(med)\n","\n","# Convert all missing values in the specified column to the default\n","def missing_default(df, name, default_value):\n","    df[name] = df[name].fillna(default_value)\n","\n","# Convert a Pandas dataframe to the X,y inputs that Keras needs\n","def to_xy(df, target):\n","    result = []\n","    for x in df.columns:\n","        if x != target:\n","            result.append(x)\n","    # find out the type of the target column.  Is it really this hard? :(\n","    target_type = df[target].dtypes\n","    target_type = target_type[0] if hasattr(\n","        target_type, '__iter__') else target_type\n","    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n","    if target_type in (np.int64, np.int32):\n","        # Classification\n","        dummies = pd.get_dummies(df[target])\n","        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n","    # Regression\n","    return df[result].values.astype(np.float32), df[[target]].values.astype(np.float32)\n","\n","# Nicely formatted time string\n","def hms_string(sec_elapsed):\n","    h = int(sec_elapsed / (60 * 60))\n","    m = int((sec_elapsed % (60 * 60)) / 60)\n","    s = sec_elapsed % 60\n","    return f\"{h}:{m:>02}:{s:>05.2f}\"\n","\n","# Regression chart.\n","def chart_regression(pred, y, sort=True):\n","    t = pd.DataFrame({'pred': pred, 'y': y.flatten()})\n","    if sort:\n","        t.sort_values(by=['y'], inplace=True)\n","    plt.plot(t['y'].tolist(), label='expected')\n","    plt.plot(t['pred'].tolist(), label='prediction')\n","    plt.ylabel('output')\n","    plt.legend()\n","    plt.show()\n","\n","# Remove all rows where the specified column is +/- sd standard deviations\n","def remove_outliers(df, name, sd):\n","    drop_rows = df.index[(np.abs(df[name] - df[name].mean())\n","                          >= (sd * df[name].std()))]\n","    df.drop(drop_rows, axis=0, inplace=True)\n","\n","# Encode a column to a range between normalized_low and normalized_high.\n","def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n","                         data_low=None, data_high=None):\n","    if data_low is None:\n","        data_low = min(df[name])\n","        data_high = max(df[name])\n","    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n","        * (normalized_high - normalized_low) + normalized_low"]},{"cell_type":"markdown","metadata":{"id":"V7P4MvlYRVUW"},"source":["Now build a neural network model for auto-mpg.  Below are several iteration of this, both trying to (informally) tune the network and manipulate the data.  The first attempt at building a neural network to predict MPG from the rest of the data is straightforward with a two layer (or single hidden layer) network.\n","\n","**Sequential** is the standard feedforward network library from Keras, with **Dense** being used for layers (that is, each output from one layer is an input to each input in the next layer).\n","\n","The code reads the csv file (located in the folder described by path), drops the name column, and fills in the missing values from horsepower, and defines X and y (I've included two ways of doing this).\n","\n","The model has a single hidden layer, defined to have 4 units, each with inputs matching the shape of the features, and a sigmoid activation function.\n","\n","The output layer has a single unit (since the problem is regression),  will take input from the output of each unit in the hidden layer, and no activation function is supplied (hence a linear combination of the outputs is computed, which is what is wanted for regression).\n","\n","The error function, the loss, is then defined to be mean squared error (as is standard for regression) and learning is using the adam tactic (usually your first choice).\n","\n","Finally, the model is fitted to the X,y data, with verbose flag indicating the output of training steps, and epochs indicates the number of epoch to run training for (here, 10).\n","\n","### Controling the amount of output\n","\n","Use the verbose flag.  You can eliminate this output by setting the verbose setting of the fit command:\n","\n","* **verbose=0** - No progress output (use with Juputer if you do not want output)\n","* **verbose=1** - Display progress bar, does not always work well with Jupyter\n","* **verbose=2** - Summary progress output (use with Jupyter if you want to know the loss at each epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tepnBQvGRVUW","outputId":"f6b8edfb-a302-4196-d0c0-cf3db299a2ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_4 (Dense)              (None, 4)                 32        \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 1)                 5         \n","=================================================================\n","Total params: 37\n","Trainable params: 37\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","import pandas as pd\n","import io\n","import os\n","import requests\n","import numpy as np\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","path = \"../ex1/\" #folder where the code is\n","\n","filename_read = os.path.join(path,\"auto-mpg.csv\")\n","df = pd.read_csv(filename_read,na_values=['NA','?'])\n","\n","cars = df['name']\n","df.drop('name',1,inplace=True)\n","missing_median(df, 'horsepower') #method call to missing_median above\n","#X,y using the diagnositic helper method above\n","X,y = to_xy(df,\"mpg\")\n","\n","#X,y directly reading the data out as numpy arrays\n","#y = df['mpg'].to_numpy()\n","#X = df.drop(columns=['mpg']).to_numpy()\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n","\n","model = Sequential()\n","model.add(Dense(4, input_shape=X[1].shape, activation='sigmoid')) # Hidden 1\n","model.add(Dense(1)) # Output\n","model.summary() #note, only works if input shape specified, or Input layer given"]},{"cell_type":"markdown","metadata":{"id":"ViL36QyXRVUX"},"source":["Now train the model for 10 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEzkfgK8RVUX","outputId":"d11efc91-408a-433b-d566-db5055696b87"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","10/10 - 0s - loss: 582.3499\n","Epoch 2/10\n","10/10 - 0s - loss: 581.4282\n","Epoch 3/10\n","10/10 - 0s - loss: 580.5265\n","Epoch 4/10\n","10/10 - 0s - loss: 579.6072\n","Epoch 5/10\n","10/10 - 0s - loss: 578.7035\n","Epoch 6/10\n","10/10 - 0s - loss: 577.7889\n","Epoch 7/10\n","10/10 - 0s - loss: 576.8870\n","Epoch 8/10\n","10/10 - 0s - loss: 575.9738\n","Epoch 9/10\n","10/10 - 0s - loss: 575.0804\n","Epoch 10/10\n","10/10 - 0s - loss: 574.1845\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_4 (Dense)              (None, 4)                 32        \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 1)                 5         \n","=================================================================\n","Total params: 37\n","Trainable params: 37\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.compile(loss='mean_squared_error', optimizer='adam')\n","model.fit(X_train,y_train,verbose=2,epochs=10)\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"06JMc4DtRVUX"},"source":["The error is rather high, and this isn't a good result.  There are a number of things that we could change:\n","- increase the number of epochs (the loss is dropping, so this could be a good choice)\n","- change the number of units in the hidden layer (maybe there's not enough scope to tune the network to the data)\n","- change the activition function\n","- add more layers\n","- possibly use a different optimizer\n","\n","First let's try more epochs, with 200."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58aWr_78RVUY","outputId":"0d1a1ef2-c10d-432e-cb1e-fe09fae33981"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/200\n","10/10 - 0s - loss: 573.5871\n","Epoch 2/200\n","10/10 - 0s - loss: 572.2319\n","Epoch 3/200\n","10/10 - 0s - loss: 570.8829\n","Epoch 4/200\n","10/10 - 0s - loss: 569.5245\n","Epoch 5/200\n","10/10 - 0s - loss: 568.1859\n","Epoch 6/200\n","10/10 - 0s - loss: 566.8389\n","Epoch 7/200\n","10/10 - 0s - loss: 565.4934\n","Epoch 8/200\n","10/10 - 0s - loss: 564.1403\n","Epoch 9/200\n","10/10 - 0s - loss: 562.8221\n","Epoch 10/200\n","10/10 - 0s - loss: 561.4672\n","Epoch 11/200\n","10/10 - 0s - loss: 560.1289\n","Epoch 12/200\n","10/10 - 0s - loss: 558.7985\n","Epoch 13/200\n","10/10 - 0s - loss: 557.4666\n","Epoch 14/200\n","10/10 - 0s - loss: 556.1339\n","Epoch 15/200\n","10/10 - 0s - loss: 554.8104\n","Epoch 16/200\n","10/10 - 0s - loss: 553.4996\n","Epoch 17/200\n","10/10 - 0s - loss: 552.1866\n","Epoch 18/200\n","10/10 - 0s - loss: 550.8715\n","Epoch 19/200\n","10/10 - 0s - loss: 549.5601\n","Epoch 20/200\n","10/10 - 0s - loss: 548.2344\n","Epoch 21/200\n","10/10 - 0s - loss: 546.9258\n","Epoch 22/200\n","10/10 - 0s - loss: 545.6234\n","Epoch 23/200\n","10/10 - 0s - loss: 544.3242\n","Epoch 24/200\n","10/10 - 0s - loss: 543.0267\n","Epoch 25/200\n","10/10 - 0s - loss: 541.7300\n","Epoch 26/200\n","10/10 - 0s - loss: 540.4261\n","Epoch 27/200\n","10/10 - 0s - loss: 539.1351\n","Epoch 28/200\n","10/10 - 0s - loss: 537.8478\n","Epoch 29/200\n","10/10 - 0s - loss: 536.5619\n","Epoch 30/200\n","10/10 - 0s - loss: 535.2897\n","Epoch 31/200\n","10/10 - 0s - loss: 534.0117\n","Epoch 32/200\n","10/10 - 0s - loss: 532.7338\n","Epoch 33/200\n","10/10 - 0s - loss: 531.4563\n","Epoch 34/200\n","10/10 - 0s - loss: 530.1780\n","Epoch 35/200\n","10/10 - 0s - loss: 528.8932\n","Epoch 36/200\n","10/10 - 0s - loss: 527.6235\n","Epoch 37/200\n","10/10 - 0s - loss: 526.3567\n","Epoch 38/200\n","10/10 - 0s - loss: 525.0935\n","Epoch 39/200\n","10/10 - 0s - loss: 523.8434\n","Epoch 40/200\n","10/10 - 0s - loss: 522.5731\n","Epoch 41/200\n","10/10 - 0s - loss: 521.3290\n","Epoch 42/200\n","10/10 - 0s - loss: 520.0704\n","Epoch 43/200\n","10/10 - 0s - loss: 518.8180\n","Epoch 44/200\n","10/10 - 0s - loss: 517.5598\n","Epoch 45/200\n","10/10 - 0s - loss: 516.3187\n","Epoch 46/200\n","10/10 - 0s - loss: 515.0742\n","Epoch 47/200\n","10/10 - 0s - loss: 513.8477\n","Epoch 48/200\n","10/10 - 0s - loss: 512.6058\n","Epoch 49/200\n","10/10 - 0s - loss: 511.3656\n","Epoch 50/200\n","10/10 - 0s - loss: 510.1241\n","Epoch 51/200\n","10/10 - 0s - loss: 508.8977\n","Epoch 52/200\n","10/10 - 0s - loss: 507.6587\n","Epoch 53/200\n","10/10 - 0s - loss: 506.4471\n","Epoch 54/200\n","10/10 - 0s - loss: 505.1973\n","Epoch 55/200\n","10/10 - 0s - loss: 503.9837\n","Epoch 56/200\n","10/10 - 0s - loss: 502.7537\n","Epoch 57/200\n","10/10 - 0s - loss: 501.5431\n","Epoch 58/200\n","10/10 - 0s - loss: 500.3320\n","Epoch 59/200\n","10/10 - 0s - loss: 499.1286\n","Epoch 60/200\n","10/10 - 0s - loss: 497.9338\n","Epoch 61/200\n","10/10 - 0s - loss: 496.7256\n","Epoch 62/200\n","10/10 - 0s - loss: 495.5345\n","Epoch 63/200\n","10/10 - 0s - loss: 494.3388\n","Epoch 64/200\n","10/10 - 0s - loss: 493.1340\n","Epoch 65/200\n","10/10 - 0s - loss: 491.9447\n","Epoch 66/200\n","10/10 - 0s - loss: 490.7304\n","Epoch 67/200\n","10/10 - 0s - loss: 489.5375\n","Epoch 68/200\n","10/10 - 0s - loss: 488.3468\n","Epoch 69/200\n","10/10 - 0s - loss: 487.1256\n","Epoch 70/200\n","10/10 - 0s - loss: 485.9558\n","Epoch 71/200\n","10/10 - 0s - loss: 484.7628\n","Epoch 72/200\n","10/10 - 0s - loss: 483.5862\n","Epoch 73/200\n","10/10 - 0s - loss: 482.4009\n","Epoch 74/200\n","10/10 - 0s - loss: 481.2388\n","Epoch 75/200\n","10/10 - 0s - loss: 480.0591\n","Epoch 76/200\n","10/10 - 0s - loss: 478.8884\n","Epoch 77/200\n","10/10 - 0s - loss: 477.7226\n","Epoch 78/200\n","10/10 - 0s - loss: 476.5665\n","Epoch 79/200\n","10/10 - 0s - loss: 475.3978\n","Epoch 80/200\n","10/10 - 0s - loss: 474.2510\n","Epoch 81/200\n","10/10 - 0s - loss: 473.0998\n","Epoch 82/200\n","10/10 - 0s - loss: 471.9467\n","Epoch 83/200\n","10/10 - 0s - loss: 470.7953\n","Epoch 84/200\n","10/10 - 0s - loss: 469.6578\n","Epoch 85/200\n","10/10 - 0s - loss: 468.5007\n","Epoch 86/200\n","10/10 - 0s - loss: 467.3508\n","Epoch 87/200\n","10/10 - 0s - loss: 466.1943\n","Epoch 88/200\n","10/10 - 0s - loss: 465.0574\n","Epoch 89/200\n","10/10 - 0s - loss: 463.9192\n","Epoch 90/200\n","10/10 - 0s - loss: 462.7753\n","Epoch 91/200\n","10/10 - 0s - loss: 461.6327\n","Epoch 92/200\n","10/10 - 0s - loss: 460.4993\n","Epoch 93/200\n","10/10 - 0s - loss: 459.3675\n","Epoch 94/200\n","10/10 - 0s - loss: 458.2482\n","Epoch 95/200\n","10/10 - 0s - loss: 457.1114\n","Epoch 96/200\n","10/10 - 0s - loss: 455.9977\n","Epoch 97/200\n","10/10 - 0s - loss: 454.8731\n","Epoch 98/200\n","10/10 - 0s - loss: 453.7488\n","Epoch 99/200\n","10/10 - 0s - loss: 452.6409\n","Epoch 100/200\n","10/10 - 0s - loss: 451.5185\n","Epoch 101/200\n","10/10 - 0s - loss: 450.4194\n","Epoch 102/200\n","10/10 - 0s - loss: 449.2957\n","Epoch 103/200\n","10/10 - 0s - loss: 448.1794\n","Epoch 104/200\n","10/10 - 0s - loss: 447.0789\n","Epoch 105/200\n","10/10 - 0s - loss: 445.9669\n","Epoch 106/200\n","10/10 - 0s - loss: 444.8558\n","Epoch 107/200\n","10/10 - 0s - loss: 443.7495\n","Epoch 108/200\n","10/10 - 0s - loss: 442.6463\n","Epoch 109/200\n","10/10 - 0s - loss: 441.5378\n","Epoch 110/200\n","10/10 - 0s - loss: 440.4277\n","Epoch 111/200\n","10/10 - 0s - loss: 439.3342\n","Epoch 112/200\n","10/10 - 0s - loss: 438.2428\n","Epoch 113/200\n","10/10 - 0s - loss: 437.1547\n","Epoch 114/200\n","10/10 - 0s - loss: 436.0773\n","Epoch 115/200\n","10/10 - 0s - loss: 434.9776\n","Epoch 116/200\n","10/10 - 0s - loss: 433.8909\n","Epoch 117/200\n","10/10 - 0s - loss: 432.8106\n","Epoch 118/200\n","10/10 - 0s - loss: 431.7060\n","Epoch 119/200\n","10/10 - 0s - loss: 430.6465\n","Epoch 120/200\n","10/10 - 0s - loss: 429.5627\n","Epoch 121/200\n","10/10 - 0s - loss: 428.4938\n","Epoch 122/200\n","10/10 - 0s - loss: 427.4102\n","Epoch 123/200\n","10/10 - 0s - loss: 426.3388\n","Epoch 124/200\n","10/10 - 0s - loss: 425.2761\n","Epoch 125/200\n","10/10 - 0s - loss: 424.2003\n","Epoch 126/200\n","10/10 - 0s - loss: 423.1354\n","Epoch 127/200\n","10/10 - 0s - loss: 422.0624\n","Epoch 128/200\n","10/10 - 0s - loss: 421.0084\n","Epoch 129/200\n","10/10 - 0s - loss: 419.9483\n","Epoch 130/200\n","10/10 - 0s - loss: 418.8972\n","Epoch 131/200\n","10/10 - 0s - loss: 417.8444\n","Epoch 132/200\n","10/10 - 0s - loss: 416.8088\n","Epoch 133/200\n","10/10 - 0s - loss: 415.7564\n","Epoch 134/200\n","10/10 - 0s - loss: 414.7253\n","Epoch 135/200\n","10/10 - 0s - loss: 413.6873\n","Epoch 136/200\n","10/10 - 0s - loss: 412.6503\n","Epoch 137/200\n","10/10 - 0s - loss: 411.5961\n","Epoch 138/200\n","10/10 - 0s - loss: 410.5711\n","Epoch 139/200\n","10/10 - 0s - loss: 409.5291\n","Epoch 140/200\n","10/10 - 0s - loss: 408.4949\n","Epoch 141/200\n","10/10 - 0s - loss: 407.4656\n","Epoch 142/200\n","10/10 - 0s - loss: 406.4264\n","Epoch 143/200\n","10/10 - 0s - loss: 405.4035\n","Epoch 144/200\n","10/10 - 0s - loss: 404.3770\n","Epoch 145/200\n","10/10 - 0s - loss: 403.3487\n","Epoch 146/200\n","10/10 - 0s - loss: 402.3578\n","Epoch 147/200\n","10/10 - 0s - loss: 401.3358\n","Epoch 148/200\n","10/10 - 0s - loss: 400.3244\n","Epoch 149/200\n","10/10 - 0s - loss: 399.3024\n","Epoch 150/200\n","10/10 - 0s - loss: 398.2899\n","Epoch 151/200\n","10/10 - 0s - loss: 397.2843\n","Epoch 152/200\n","10/10 - 0s - loss: 396.2850\n","Epoch 153/200\n","10/10 - 0s - loss: 395.2881\n","Epoch 154/200\n","10/10 - 0s - loss: 394.2813\n","Epoch 155/200\n","10/10 - 0s - loss: 393.2927\n","Epoch 156/200\n","10/10 - 0s - loss: 392.2962\n","Epoch 157/200\n","10/10 - 0s - loss: 391.2940\n","Epoch 158/200\n","10/10 - 0s - loss: 390.3011\n","Epoch 159/200\n","10/10 - 0s - loss: 389.3037\n","Epoch 160/200\n","10/10 - 0s - loss: 388.3196\n","Epoch 161/200\n","10/10 - 0s - loss: 387.3042\n","Epoch 162/200\n","10/10 - 0s - loss: 386.3292\n","Epoch 163/200\n","10/10 - 0s - loss: 385.3279\n","Epoch 164/200\n","10/10 - 0s - loss: 384.3398\n","Epoch 165/200\n","10/10 - 0s - loss: 383.3393\n","Epoch 166/200\n","10/10 - 0s - loss: 382.3618\n","Epoch 167/200\n","10/10 - 0s - loss: 381.3772\n","Epoch 168/200\n","10/10 - 0s - loss: 380.3944\n","Epoch 169/200\n","10/10 - 0s - loss: 379.4322\n","Epoch 170/200\n","10/10 - 0s - loss: 378.4688\n","Epoch 171/200\n","10/10 - 0s - loss: 377.4815\n","Epoch 172/200\n","10/10 - 0s - loss: 376.5208\n","Epoch 173/200\n","10/10 - 0s - loss: 375.5493\n","Epoch 174/200\n","10/10 - 0s - loss: 374.5804\n","Epoch 175/200\n","10/10 - 0s - loss: 373.6157\n","Epoch 176/200\n","10/10 - 0s - loss: 372.6605\n","Epoch 177/200\n","10/10 - 0s - loss: 371.6845\n","Epoch 178/200\n","10/10 - 0s - loss: 370.7235\n","Epoch 179/200\n","10/10 - 0s - loss: 369.7680\n","Epoch 180/200\n","10/10 - 0s - loss: 368.8058\n","Epoch 181/200\n","10/10 - 0s - loss: 367.8615\n","Epoch 182/200\n","10/10 - 0s - loss: 366.9166\n","Epoch 183/200\n","10/10 - 0s - loss: 365.9493\n","Epoch 184/200\n","10/10 - 0s - loss: 365.0157\n","Epoch 185/200\n","10/10 - 0s - loss: 364.0620\n","Epoch 186/200\n","10/10 - 0s - loss: 363.1309\n","Epoch 187/200\n","10/10 - 0s - loss: 362.2000\n","Epoch 188/200\n","10/10 - 0s - loss: 361.2549\n","Epoch 189/200\n","10/10 - 0s - loss: 360.3255\n","Epoch 190/200\n","10/10 - 0s - loss: 359.3956\n","Epoch 191/200\n","10/10 - 0s - loss: 358.4649\n","Epoch 192/200\n","10/10 - 0s - loss: 357.5328\n","Epoch 193/200\n","10/10 - 0s - loss: 356.5935\n","Epoch 194/200\n","10/10 - 0s - loss: 355.6623\n","Epoch 195/200\n","10/10 - 0s - loss: 354.7233\n","Epoch 196/200\n","10/10 - 0s - loss: 353.7971\n","Epoch 197/200\n","10/10 - 0s - loss: 352.8798\n","Epoch 198/200\n","10/10 - 0s - loss: 351.9494\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 199/200\n","10/10 - 0s - loss: 351.0349\n","Epoch 200/200\n","10/10 - 0s - loss: 350.0991\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fa6affb0490>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["model = Sequential()\n","model.add(Dense(4, input_dim=X.shape[1], activation='sigmoid')) # Hidden 1\n","model.add(Dense(1)) # Output\n","model.compile(loss='mean_squared_error', optimizer='adam')\n","model.fit(X_train,y_train,verbose=2,epochs=200)"]},{"cell_type":"markdown","metadata":{"id":"szVbdEx1RVUY"},"source":["The results appears to be somewhat better, but not good.  We could add more epochs, but perhaps that isn't the problem.\n","\n","So next, let's try more units in the hidden layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTpJG7rwRVUY","outputId":"5a20322f-10aa-46dc-845c-7b2928d5d49b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/200\n","10/10 - 0s - loss: 620.2162\n","Epoch 2/200\n","10/10 - 0s - loss: 605.2075\n","Epoch 3/200\n","10/10 - 0s - loss: 594.4656\n","Epoch 4/200\n","10/10 - 0s - loss: 585.0262\n","Epoch 5/200\n","10/10 - 0s - loss: 576.0746\n","Epoch 6/200\n","10/10 - 0s - loss: 567.2525\n","Epoch 7/200\n","10/10 - 0s - loss: 558.4871\n","Epoch 8/200\n","10/10 - 0s - loss: 549.8723\n","Epoch 9/200\n","10/10 - 0s - loss: 541.2425\n","Epoch 10/200\n","10/10 - 0s - loss: 532.8395\n","Epoch 11/200\n","10/10 - 0s - loss: 524.3641\n","Epoch 12/200\n","10/10 - 0s - loss: 516.1589\n","Epoch 13/200\n","10/10 - 0s - loss: 508.0172\n","Epoch 14/200\n","10/10 - 0s - loss: 499.9906\n","Epoch 15/200\n","10/10 - 0s - loss: 492.1070\n","Epoch 16/200\n","10/10 - 0s - loss: 484.3630\n","Epoch 17/200\n","10/10 - 0s - loss: 476.6146\n","Epoch 18/200\n","10/10 - 0s - loss: 469.1113\n","Epoch 19/200\n","10/10 - 0s - loss: 461.5978\n","Epoch 20/200\n","10/10 - 0s - loss: 454.2456\n","Epoch 21/200\n","10/10 - 0s - loss: 446.9401\n","Epoch 22/200\n","10/10 - 0s - loss: 439.8298\n","Epoch 23/200\n","10/10 - 0s - loss: 432.6684\n","Epoch 24/200\n","10/10 - 0s - loss: 425.7211\n","Epoch 25/200\n","10/10 - 0s - loss: 418.8154\n","Epoch 26/200\n","10/10 - 0s - loss: 412.0606\n","Epoch 27/200\n","10/10 - 0s - loss: 405.3824\n","Epoch 28/200\n","10/10 - 0s - loss: 398.7893\n","Epoch 29/200\n","10/10 - 0s - loss: 392.2684\n","Epoch 30/200\n","10/10 - 0s - loss: 385.9391\n","Epoch 31/200\n","10/10 - 0s - loss: 379.6768\n","Epoch 32/200\n","10/10 - 0s - loss: 373.4662\n","Epoch 33/200\n","10/10 - 0s - loss: 367.3453\n","Epoch 34/200\n","10/10 - 0s - loss: 361.3091\n","Epoch 35/200\n","10/10 - 0s - loss: 355.3503\n","Epoch 36/200\n","10/10 - 0s - loss: 349.5256\n","Epoch 37/200\n","10/10 - 0s - loss: 343.8201\n","Epoch 38/200\n","10/10 - 0s - loss: 338.1292\n","Epoch 39/200\n","10/10 - 0s - loss: 332.5000\n","Epoch 40/200\n","10/10 - 0s - loss: 327.0483\n","Epoch 41/200\n","10/10 - 0s - loss: 321.6098\n","Epoch 42/200\n","10/10 - 0s - loss: 316.2614\n","Epoch 43/200\n","10/10 - 0s - loss: 311.0957\n","Epoch 44/200\n","10/10 - 0s - loss: 305.8343\n","Epoch 45/200\n","10/10 - 0s - loss: 300.7741\n","Epoch 46/200\n","10/10 - 0s - loss: 295.7970\n","Epoch 47/200\n","10/10 - 0s - loss: 290.8861\n","Epoch 48/200\n","10/10 - 0s - loss: 286.1081\n","Epoch 49/200\n","10/10 - 0s - loss: 281.2730\n","Epoch 50/200\n","10/10 - 0s - loss: 276.5385\n","Epoch 51/200\n","10/10 - 0s - loss: 271.8936\n","Epoch 52/200\n","10/10 - 0s - loss: 267.3626\n","Epoch 53/200\n","10/10 - 0s - loss: 262.9317\n","Epoch 54/200\n","10/10 - 0s - loss: 258.5096\n","Epoch 55/200\n","10/10 - 0s - loss: 254.1763\n","Epoch 56/200\n","10/10 - 0s - loss: 249.9952\n","Epoch 57/200\n","10/10 - 0s - loss: 245.8429\n","Epoch 58/200\n","10/10 - 0s - loss: 241.7551\n","Epoch 59/200\n","10/10 - 0s - loss: 237.7881\n","Epoch 60/200\n","10/10 - 0s - loss: 233.8467\n","Epoch 61/200\n","10/10 - 0s - loss: 229.9678\n","Epoch 62/200\n","10/10 - 0s - loss: 226.1128\n","Epoch 63/200\n","10/10 - 0s - loss: 222.4078\n","Epoch 64/200\n","10/10 - 0s - loss: 218.6544\n","Epoch 65/200\n","10/10 - 0s - loss: 215.1457\n","Epoch 66/200\n","10/10 - 0s - loss: 211.4781\n","Epoch 67/200\n","10/10 - 0s - loss: 207.9721\n","Epoch 68/200\n","10/10 - 0s - loss: 204.5567\n","Epoch 69/200\n","10/10 - 0s - loss: 201.1161\n","Epoch 70/200\n","10/10 - 0s - loss: 197.8521\n","Epoch 71/200\n","10/10 - 0s - loss: 194.5750\n","Epoch 72/200\n","10/10 - 0s - loss: 191.3443\n","Epoch 73/200\n","10/10 - 0s - loss: 188.3301\n","Epoch 74/200\n","10/10 - 0s - loss: 185.2195\n","Epoch 75/200\n","10/10 - 0s - loss: 182.1881\n","Epoch 76/200\n","10/10 - 0s - loss: 179.2354\n","Epoch 77/200\n","10/10 - 0s - loss: 176.2825\n","Epoch 78/200\n","10/10 - 0s - loss: 173.5143\n","Epoch 79/200\n","10/10 - 0s - loss: 170.6694\n","Epoch 80/200\n","10/10 - 0s - loss: 168.0486\n","Epoch 81/200\n","10/10 - 0s - loss: 165.3379\n","Epoch 82/200\n","10/10 - 0s - loss: 162.6711\n","Epoch 83/200\n","10/10 - 0s - loss: 160.1448\n","Epoch 84/200\n","10/10 - 0s - loss: 157.6177\n","Epoch 85/200\n","10/10 - 0s - loss: 155.1661\n","Epoch 86/200\n","10/10 - 0s - loss: 152.7319\n","Epoch 87/200\n","10/10 - 0s - loss: 150.3476\n","Epoch 88/200\n","10/10 - 0s - loss: 148.0346\n","Epoch 89/200\n","10/10 - 0s - loss: 145.7881\n","Epoch 90/200\n","10/10 - 0s - loss: 143.5538\n","Epoch 91/200\n","10/10 - 0s - loss: 141.3505\n","Epoch 92/200\n","10/10 - 0s - loss: 139.1979\n","Epoch 93/200\n","10/10 - 0s - loss: 137.0977\n","Epoch 94/200\n","10/10 - 0s - loss: 135.0463\n","Epoch 95/200\n","10/10 - 0s - loss: 133.0914\n","Epoch 96/200\n","10/10 - 0s - loss: 131.0663\n","Epoch 97/200\n","10/10 - 0s - loss: 129.1950\n","Epoch 98/200\n","10/10 - 0s - loss: 127.2889\n","Epoch 99/200\n","10/10 - 0s - loss: 125.5303\n","Epoch 100/200\n","10/10 - 0s - loss: 123.7215\n","Epoch 101/200\n","10/10 - 0s - loss: 121.9687\n","Epoch 102/200\n","10/10 - 0s - loss: 120.3335\n","Epoch 103/200\n","10/10 - 0s - loss: 118.6750\n","Epoch 104/200\n","10/10 - 0s - loss: 117.0771\n","Epoch 105/200\n","10/10 - 0s - loss: 115.4689\n","Epoch 106/200\n","10/10 - 0s - loss: 113.9598\n","Epoch 107/200\n","10/10 - 0s - loss: 112.4145\n","Epoch 108/200\n","10/10 - 0s - loss: 110.8978\n","Epoch 109/200\n","10/10 - 0s - loss: 109.4935\n","Epoch 110/200\n","10/10 - 0s - loss: 108.0720\n","Epoch 111/200\n","10/10 - 0s - loss: 106.6822\n","Epoch 112/200\n","10/10 - 0s - loss: 105.3781\n","Epoch 113/200\n","10/10 - 0s - loss: 104.0705\n","Epoch 114/200\n","10/10 - 0s - loss: 102.7797\n","Epoch 115/200\n","10/10 - 0s - loss: 101.5765\n","Epoch 116/200\n","10/10 - 0s - loss: 100.3403\n","Epoch 117/200\n","10/10 - 0s - loss: 99.1203\n","Epoch 118/200\n","10/10 - 0s - loss: 97.9419\n","Epoch 119/200\n","10/10 - 0s - loss: 96.7439\n","Epoch 120/200\n","10/10 - 0s - loss: 95.6946\n","Epoch 121/200\n","10/10 - 0s - loss: 94.5141\n","Epoch 122/200\n","10/10 - 0s - loss: 93.4884\n","Epoch 123/200\n","10/10 - 0s - loss: 92.4360\n","Epoch 124/200\n","10/10 - 0s - loss: 91.4112\n","Epoch 125/200\n","10/10 - 0s - loss: 90.4194\n","Epoch 126/200\n","10/10 - 0s - loss: 89.4576\n","Epoch 127/200\n","10/10 - 0s - loss: 88.5232\n","Epoch 128/200\n","10/10 - 0s - loss: 87.6286\n","Epoch 129/200\n","10/10 - 0s - loss: 86.7239\n","Epoch 130/200\n","10/10 - 0s - loss: 85.9013\n","Epoch 131/200\n","10/10 - 0s - loss: 85.0969\n","Epoch 132/200\n","10/10 - 0s - loss: 84.2862\n","Epoch 133/200\n","10/10 - 0s - loss: 83.5377\n","Epoch 134/200\n","10/10 - 0s - loss: 82.8262\n","Epoch 135/200\n","10/10 - 0s - loss: 82.0828\n","Epoch 136/200\n","10/10 - 0s - loss: 81.3496\n","Epoch 137/200\n","10/10 - 0s - loss: 80.6683\n","Epoch 138/200\n","10/10 - 0s - loss: 79.9650\n","Epoch 139/200\n","10/10 - 0s - loss: 79.3343\n","Epoch 140/200\n","10/10 - 0s - loss: 78.6950\n","Epoch 141/200\n","10/10 - 0s - loss: 78.0883\n","Epoch 142/200\n","10/10 - 0s - loss: 77.4607\n","Epoch 143/200\n","10/10 - 0s - loss: 76.8769\n","Epoch 144/200\n","10/10 - 0s - loss: 76.3001\n","Epoch 145/200\n","10/10 - 0s - loss: 75.7405\n","Epoch 146/200\n","10/10 - 0s - loss: 75.1671\n","Epoch 147/200\n","10/10 - 0s - loss: 74.6480\n","Epoch 148/200\n","10/10 - 0s - loss: 74.1649\n","Epoch 149/200\n","10/10 - 0s - loss: 73.7099\n","Epoch 150/200\n","10/10 - 0s - loss: 73.2405\n","Epoch 151/200\n","10/10 - 0s - loss: 72.7805\n","Epoch 152/200\n","10/10 - 0s - loss: 72.3021\n","Epoch 153/200\n","10/10 - 0s - loss: 71.9029\n","Epoch 154/200\n","10/10 - 0s - loss: 71.4662\n","Epoch 155/200\n","10/10 - 0s - loss: 71.0273\n","Epoch 156/200\n","10/10 - 0s - loss: 70.6303\n","Epoch 157/200\n","10/10 - 0s - loss: 70.2155\n","Epoch 158/200\n","10/10 - 0s - loss: 69.8401\n","Epoch 159/200\n","10/10 - 0s - loss: 69.4889\n","Epoch 160/200\n","10/10 - 0s - loss: 69.1292\n","Epoch 161/200\n","10/10 - 0s - loss: 68.7823\n","Epoch 162/200\n","10/10 - 0s - loss: 68.4611\n","Epoch 163/200\n","10/10 - 0s - loss: 68.1464\n","Epoch 164/200\n","10/10 - 0s - loss: 67.8500\n","Epoch 165/200\n","10/10 - 0s - loss: 67.5502\n","Epoch 166/200\n","10/10 - 0s - loss: 67.2962\n","Epoch 167/200\n","10/10 - 0s - loss: 67.0012\n","Epoch 168/200\n","10/10 - 0s - loss: 66.7419\n","Epoch 169/200\n","10/10 - 0s - loss: 66.4807\n","Epoch 170/200\n","10/10 - 0s - loss: 66.2399\n","Epoch 171/200\n","10/10 - 0s - loss: 65.9874\n","Epoch 172/200\n","10/10 - 0s - loss: 65.7591\n","Epoch 173/200\n","10/10 - 0s - loss: 65.4909\n","Epoch 174/200\n","10/10 - 0s - loss: 65.2988\n","Epoch 175/200\n","10/10 - 0s - loss: 65.0850\n","Epoch 176/200\n","10/10 - 0s - loss: 64.8738\n","Epoch 177/200\n","10/10 - 0s - loss: 64.6903\n","Epoch 178/200\n","10/10 - 0s - loss: 64.5042\n","Epoch 179/200\n","10/10 - 0s - loss: 64.3126\n","Epoch 180/200\n","10/10 - 0s - loss: 64.1382\n","Epoch 181/200\n","10/10 - 0s - loss: 63.9637\n","Epoch 182/200\n","10/10 - 0s - loss: 63.8053\n","Epoch 183/200\n","10/10 - 0s - loss: 63.6865\n","Epoch 184/200\n","10/10 - 0s - loss: 63.5326\n","Epoch 185/200\n","10/10 - 0s - loss: 63.4100\n","Epoch 186/200\n","10/10 - 0s - loss: 63.2796\n","Epoch 187/200\n","10/10 - 0s - loss: 63.1487\n","Epoch 188/200\n","10/10 - 0s - loss: 63.0231\n","Epoch 189/200\n","10/10 - 0s - loss: 62.9125\n","Epoch 190/200\n","10/10 - 0s - loss: 62.7977\n","Epoch 191/200\n","10/10 - 0s - loss: 62.6842\n","Epoch 192/200\n","10/10 - 0s - loss: 62.5729\n","Epoch 193/200\n","10/10 - 0s - loss: 62.4869\n","Epoch 194/200\n","10/10 - 0s - loss: 62.3955\n","Epoch 195/200\n","10/10 - 0s - loss: 62.2941\n","Epoch 196/200\n","10/10 - 0s - loss: 62.2033\n","Epoch 197/200\n","10/10 - 0s - loss: 62.1144\n","Epoch 198/200\n","10/10 - 0s - loss: 62.0329\n","Epoch 199/200\n","10/10 - 0s - loss: 61.9578\n","Epoch 200/200\n","10/10 - 0s - loss: 61.8754\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fa6afa97fd0>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["model = Sequential()\n","model.add(Dense(32, input_dim=X.shape[1], activation='sigmoid')) # Hidden 1\n","model.add(Dense(1)) # Output\n","model.compile(loss='mean_squared_error', optimizer='adam')\n","model.fit(X_train,y_train,verbose=2,epochs=200)"]},{"cell_type":"markdown","metadata":{"id":"KDN00_24RVUY"},"source":["Again, this is an improvement.  We could try a lot more units."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77WRulnpRVUZ","outputId":"d23b056d-3008-436d-9703-69fbea62d49a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/200\n","13/13 - 0s - loss: 437.0599\n","Epoch 2/200\n","13/13 - 0s - loss: 194.3474\n","Epoch 3/200\n","13/13 - 0s - loss: 95.4476\n","Epoch 4/200\n","13/13 - 0s - loss: 65.9695\n","Epoch 5/200\n","13/13 - 0s - loss: 60.5927\n","Epoch 6/200\n","13/13 - 0s - loss: 61.0416\n","Epoch 7/200\n","13/13 - 0s - loss: 60.6898\n","Epoch 8/200\n","13/13 - 0s - loss: 60.4298\n","Epoch 9/200\n","13/13 - 0s - loss: 60.3439\n","Epoch 10/200\n","13/13 - 0s - loss: 60.3075\n","Epoch 11/200\n","13/13 - 0s - loss: 60.1167\n","Epoch 12/200\n","13/13 - 0s - loss: 60.0193\n","Epoch 13/200\n","13/13 - 0s - loss: 59.8972\n","Epoch 14/200\n","13/13 - 0s - loss: 59.7999\n","Epoch 15/200\n","13/13 - 0s - loss: 59.7262\n","Epoch 16/200\n","13/13 - 0s - loss: 59.6829\n","Epoch 17/200\n","13/13 - 0s - loss: 59.5031\n","Epoch 18/200\n","13/13 - 0s - loss: 59.4825\n","Epoch 19/200\n","13/13 - 0s - loss: 59.4301\n","Epoch 20/200\n","13/13 - 0s - loss: 59.3156\n","Epoch 21/200\n","13/13 - 0s - loss: 58.7917\n","Epoch 22/200\n","13/13 - 0s - loss: 58.5312\n","Epoch 23/200\n","13/13 - 0s - loss: 58.2196\n","Epoch 24/200\n","13/13 - 0s - loss: 58.0219\n","Epoch 25/200\n","13/13 - 0s - loss: 57.7623\n","Epoch 26/200\n","13/13 - 0s - loss: 57.5576\n","Epoch 27/200\n","13/13 - 0s - loss: 57.3860\n","Epoch 28/200\n","13/13 - 0s - loss: 57.0714\n","Epoch 29/200\n","13/13 - 0s - loss: 57.0464\n","Epoch 30/200\n","13/13 - 0s - loss: 56.5651\n","Epoch 31/200\n","13/13 - 0s - loss: 56.7242\n","Epoch 32/200\n","13/13 - 0s - loss: 56.5348\n","Epoch 33/200\n","13/13 - 0s - loss: 56.2116\n","Epoch 34/200\n","13/13 - 0s - loss: 55.9787\n","Epoch 35/200\n","13/13 - 0s - loss: 55.7855\n","Epoch 36/200\n","13/13 - 0s - loss: 56.0004\n","Epoch 37/200\n","13/13 - 0s - loss: 55.3351\n","Epoch 38/200\n","13/13 - 0s - loss: 55.3689\n","Epoch 39/200\n","13/13 - 0s - loss: 55.2527\n","Epoch 40/200\n","13/13 - 0s - loss: 55.1815\n","Epoch 41/200\n","13/13 - 0s - loss: 55.3098\n","Epoch 42/200\n","13/13 - 0s - loss: 54.6464\n","Epoch 43/200\n","13/13 - 0s - loss: 54.5150\n","Epoch 44/200\n","13/13 - 0s - loss: 54.4623\n","Epoch 45/200\n","13/13 - 0s - loss: 54.2781\n","Epoch 46/200\n","13/13 - 0s - loss: 54.0373\n","Epoch 47/200\n","13/13 - 0s - loss: 53.8507\n","Epoch 48/200\n","13/13 - 0s - loss: 53.8291\n","Epoch 49/200\n","13/13 - 0s - loss: 53.5673\n","Epoch 50/200\n","13/13 - 0s - loss: 53.5743\n","Epoch 51/200\n","13/13 - 0s - loss: 53.6792\n","Epoch 52/200\n","13/13 - 0s - loss: 53.2497\n","Epoch 53/200\n","13/13 - 0s - loss: 53.1074\n","Epoch 54/200\n","13/13 - 0s - loss: 53.0326\n","Epoch 55/200\n","13/13 - 0s - loss: 52.7635\n","Epoch 56/200\n","13/13 - 0s - loss: 52.6751\n","Epoch 57/200\n","13/13 - 0s - loss: 52.3748\n","Epoch 58/200\n","13/13 - 0s - loss: 52.4592\n","Epoch 59/200\n","13/13 - 0s - loss: 52.3294\n","Epoch 60/200\n","13/13 - 0s - loss: 52.3451\n","Epoch 61/200\n","13/13 - 0s - loss: 52.0932\n","Epoch 62/200\n","13/13 - 0s - loss: 52.0344\n","Epoch 63/200\n","13/13 - 0s - loss: 51.7876\n","Epoch 64/200\n","13/13 - 0s - loss: 51.5390\n","Epoch 65/200\n","13/13 - 0s - loss: 51.4374\n","Epoch 66/200\n","13/13 - 0s - loss: 51.4164\n","Epoch 67/200\n","13/13 - 0s - loss: 51.1307\n","Epoch 68/200\n","13/13 - 0s - loss: 51.2333\n","Epoch 69/200\n","13/13 - 0s - loss: 51.1269\n","Epoch 70/200\n","13/13 - 0s - loss: 50.9107\n","Epoch 71/200\n","13/13 - 0s - loss: 50.6828\n","Epoch 72/200\n","13/13 - 0s - loss: 51.0135\n","Epoch 73/200\n","13/13 - 0s - loss: 51.0677\n","Epoch 74/200\n","13/13 - 0s - loss: 50.2251\n","Epoch 75/200\n","13/13 - 0s - loss: 50.2221\n","Epoch 76/200\n","13/13 - 0s - loss: 49.9742\n","Epoch 77/200\n","13/13 - 0s - loss: 50.0127\n","Epoch 78/200\n","13/13 - 0s - loss: 49.9048\n","Epoch 79/200\n","13/13 - 0s - loss: 49.7533\n","Epoch 80/200\n","13/13 - 0s - loss: 49.5891\n","Epoch 81/200\n","13/13 - 0s - loss: 49.4399\n","Epoch 82/200\n","13/13 - 0s - loss: 49.6815\n","Epoch 83/200\n","13/13 - 0s - loss: 49.5169\n","Epoch 84/200\n","13/13 - 0s - loss: 49.2622\n","Epoch 85/200\n","13/13 - 0s - loss: 48.9877\n","Epoch 86/200\n","13/13 - 0s - loss: 48.9588\n","Epoch 87/200\n","13/13 - 0s - loss: 48.7882\n","Epoch 88/200\n","13/13 - 0s - loss: 48.8212\n","Epoch 89/200\n","13/13 - 0s - loss: 48.7991\n","Epoch 90/200\n","13/13 - 0s - loss: 48.4145\n","Epoch 91/200\n","13/13 - 0s - loss: 48.4758\n","Epoch 92/200\n","13/13 - 0s - loss: 48.1897\n","Epoch 93/200\n","13/13 - 0s - loss: 48.2505\n","Epoch 94/200\n","13/13 - 0s - loss: 47.9941\n","Epoch 95/200\n","13/13 - 0s - loss: 48.3778\n","Epoch 96/200\n","13/13 - 0s - loss: 47.8780\n","Epoch 97/200\n","13/13 - 0s - loss: 47.6499\n","Epoch 98/200\n","13/13 - 0s - loss: 47.7635\n","Epoch 99/200\n","13/13 - 0s - loss: 47.3573\n","Epoch 100/200\n","13/13 - 0s - loss: 47.2355\n","Epoch 101/200\n","13/13 - 0s - loss: 47.1673\n","Epoch 102/200\n","13/13 - 0s - loss: 47.1975\n","Epoch 103/200\n","13/13 - 0s - loss: 46.9233\n","Epoch 104/200\n","13/13 - 0s - loss: 46.7786\n","Epoch 105/200\n","13/13 - 0s - loss: 47.0914\n","Epoch 106/200\n","13/13 - 0s - loss: 46.5845\n","Epoch 107/200\n","13/13 - 0s - loss: 46.9404\n","Epoch 108/200\n","13/13 - 0s - loss: 46.3971\n","Epoch 109/200\n","13/13 - 0s - loss: 46.2380\n","Epoch 110/200\n","13/13 - 0s - loss: 46.4187\n","Epoch 111/200\n","13/13 - 0s - loss: 46.1452\n","Epoch 112/200\n","13/13 - 0s - loss: 46.2278\n","Epoch 113/200\n","13/13 - 0s - loss: 46.2434\n","Epoch 114/200\n","13/13 - 0s - loss: 46.1814\n","Epoch 115/200\n","13/13 - 0s - loss: 46.3633\n","Epoch 116/200\n","13/13 - 0s - loss: 45.9610\n","Epoch 117/200\n","13/13 - 0s - loss: 45.5781\n","Epoch 118/200\n","13/13 - 0s - loss: 45.6437\n","Epoch 119/200\n","13/13 - 0s - loss: 45.1041\n","Epoch 120/200\n","13/13 - 0s - loss: 45.1056\n","Epoch 121/200\n","13/13 - 0s - loss: 44.9992\n","Epoch 122/200\n","13/13 - 0s - loss: 44.9360\n","Epoch 123/200\n","13/13 - 0s - loss: 45.1709\n","Epoch 124/200\n","13/13 - 0s - loss: 45.3116\n","Epoch 125/200\n","13/13 - 0s - loss: 44.7811\n","Epoch 126/200\n","13/13 - 0s - loss: 44.9771\n","Epoch 127/200\n","13/13 - 0s - loss: 44.8100\n","Epoch 128/200\n","13/13 - 0s - loss: 44.4017\n","Epoch 129/200\n","13/13 - 0s - loss: 44.0946\n","Epoch 130/200\n","13/13 - 0s - loss: 44.0418\n","Epoch 131/200\n","13/13 - 0s - loss: 44.0394\n","Epoch 132/200\n","13/13 - 0s - loss: 43.7970\n","Epoch 133/200\n","13/13 - 0s - loss: 43.7190\n","Epoch 134/200\n","13/13 - 0s - loss: 43.8785\n","Epoch 135/200\n","13/13 - 0s - loss: 43.5371\n","Epoch 136/200\n","13/13 - 0s - loss: 43.5775\n","Epoch 137/200\n","13/13 - 0s - loss: 43.7222\n","Epoch 138/200\n","13/13 - 0s - loss: 43.3072\n","Epoch 139/200\n","13/13 - 0s - loss: 43.0697\n","Epoch 140/200\n","13/13 - 0s - loss: 43.0956\n","Epoch 141/200\n","13/13 - 0s - loss: 42.9286\n","Epoch 142/200\n","13/13 - 0s - loss: 42.9465\n","Epoch 143/200\n","13/13 - 0s - loss: 42.9427\n","Epoch 144/200\n","13/13 - 0s - loss: 43.2846\n","Epoch 145/200\n","13/13 - 0s - loss: 44.0177\n","Epoch 146/200\n","13/13 - 0s - loss: 42.8667\n","Epoch 147/200\n","13/13 - 0s - loss: 42.8421\n","Epoch 148/200\n","13/13 - 0s - loss: 42.3053\n","Epoch 149/200\n","13/13 - 0s - loss: 42.1527\n","Epoch 150/200\n","13/13 - 0s - loss: 42.1172\n","Epoch 151/200\n","13/13 - 0s - loss: 42.2029\n","Epoch 152/200\n","13/13 - 0s - loss: 42.2475\n","Epoch 153/200\n","13/13 - 0s - loss: 41.8983\n","Epoch 154/200\n","13/13 - 0s - loss: 41.5507\n","Epoch 155/200\n","13/13 - 0s - loss: 42.1227\n","Epoch 156/200\n","13/13 - 0s - loss: 41.7940\n","Epoch 157/200\n","13/13 - 0s - loss: 41.7728\n","Epoch 158/200\n","13/13 - 0s - loss: 41.8289\n","Epoch 159/200\n","13/13 - 0s - loss: 41.1852\n","Epoch 160/200\n","13/13 - 0s - loss: 41.2631\n","Epoch 161/200\n","13/13 - 0s - loss: 41.2626\n","Epoch 162/200\n","13/13 - 0s - loss: 41.1934\n","Epoch 163/200\n","13/13 - 0s - loss: 40.9345\n","Epoch 164/200\n","13/13 - 0s - loss: 41.2707\n","Epoch 165/200\n","13/13 - 0s - loss: 40.8660\n","Epoch 166/200\n","13/13 - 0s - loss: 40.7218\n","Epoch 167/200\n","13/13 - 0s - loss: 40.6448\n","Epoch 168/200\n","13/13 - 0s - loss: 40.3719\n","Epoch 169/200\n","13/13 - 0s - loss: 40.6409\n","Epoch 170/200\n","13/13 - 0s - loss: 40.3582\n","Epoch 171/200\n","13/13 - 0s - loss: 40.2951\n","Epoch 172/200\n","13/13 - 0s - loss: 40.2618\n","Epoch 173/200\n","13/13 - 0s - loss: 40.2417\n","Epoch 174/200\n","13/13 - 0s - loss: 40.1096\n","Epoch 175/200\n","13/13 - 0s - loss: 39.9772\n","Epoch 176/200\n","13/13 - 0s - loss: 39.5395\n","Epoch 177/200\n","13/13 - 0s - loss: 39.7464\n","Epoch 178/200\n","13/13 - 0s - loss: 39.6343\n","Epoch 179/200\n","13/13 - 0s - loss: 39.7403\n","Epoch 180/200\n","13/13 - 0s - loss: 40.3418\n","Epoch 181/200\n","13/13 - 0s - loss: 39.8060\n","Epoch 182/200\n","13/13 - 0s - loss: 39.3369\n","Epoch 183/200\n","13/13 - 0s - loss: 39.0750\n","Epoch 184/200\n","13/13 - 0s - loss: 39.2623\n","Epoch 185/200\n","13/13 - 0s - loss: 39.0239\n","Epoch 186/200\n","13/13 - 0s - loss: 38.7084\n","Epoch 187/200\n","13/13 - 0s - loss: 39.1934\n","Epoch 188/200\n","13/13 - 0s - loss: 38.6756\n","Epoch 189/200\n","13/13 - 0s - loss: 38.8778\n","Epoch 190/200\n","13/13 - 0s - loss: 38.6845\n","Epoch 191/200\n","13/13 - 0s - loss: 38.9495\n","Epoch 192/200\n","13/13 - 0s - loss: 38.5208\n","Epoch 193/200\n","13/13 - 0s - loss: 38.2331\n","Epoch 194/200\n","13/13 - 0s - loss: 38.0726\n","Epoch 195/200\n","13/13 - 0s - loss: 38.0141\n","Epoch 196/200\n","13/13 - 0s - loss: 37.9352\n","Epoch 197/200\n","13/13 - 0s - loss: 38.2371\n","Epoch 198/200\n","13/13 - 0s - loss: 37.8598\n","Epoch 199/200\n","13/13 - 0s - loss: 37.6181\n","Epoch 200/200\n","13/13 - 0s - loss: 37.7221\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fa6afa9b7c0>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["model = Sequential()\n","model.add(Dense(1024, input_dim=X.shape[1], activation='sigmoid')) # Hidden 1\n","model.add(Dense(1)) # Output\n","model.compile(loss='mean_squared_error', optimizer='adam')\n","model.fit(X,y,verbose=2,epochs=200)"]},{"cell_type":"markdown","metadata":{"id":"EqJNL_thRVUZ"},"source":["This gives another improvement, but there is the danger of overfitting.  Let's experiment with  a different activation function.  Let's try rectified linear units."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGzfEQQDRVUZ","outputId":"fab90c7a-6081-48a3-ede7-2ec3034c8c80"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/200\n","10/10 - 0s - loss: 5720.9863\n","Epoch 2/200\n","10/10 - 0s - loss: 1754.6990\n","Epoch 3/200\n","10/10 - 0s - loss: 935.4927\n","Epoch 4/200\n","10/10 - 0s - loss: 438.2071\n","Epoch 5/200\n","10/10 - 0s - loss: 413.0261\n","Epoch 6/200\n","10/10 - 0s - loss: 297.6965\n","Epoch 7/200\n","10/10 - 0s - loss: 158.1299\n","Epoch 8/200\n","10/10 - 0s - loss: 105.8655\n","Epoch 9/200\n","10/10 - 0s - loss: 88.8117\n","Epoch 10/200\n","10/10 - 0s - loss: 75.9348\n","Epoch 11/200\n","10/10 - 0s - loss: 66.6568\n","Epoch 12/200\n","10/10 - 0s - loss: 65.9367\n","Epoch 13/200\n","10/10 - 0s - loss: 54.2123\n","Epoch 14/200\n","10/10 - 0s - loss: 49.2195\n","Epoch 15/200\n","10/10 - 0s - loss: 44.7212\n","Epoch 16/200\n","10/10 - 0s - loss: 41.9780\n","Epoch 17/200\n","10/10 - 0s - loss: 39.4044\n","Epoch 18/200\n","10/10 - 0s - loss: 38.2757\n","Epoch 19/200\n","10/10 - 0s - loss: 36.0393\n","Epoch 20/200\n","10/10 - 0s - loss: 35.9760\n","Epoch 21/200\n","10/10 - 0s - loss: 37.1670\n","Epoch 22/200\n","10/10 - 0s - loss: 30.1459\n","Epoch 23/200\n","10/10 - 0s - loss: 37.4788\n","Epoch 24/200\n","10/10 - 0s - loss: 32.1469\n","Epoch 25/200\n","10/10 - 0s - loss: 29.3595\n","Epoch 26/200\n","10/10 - 0s - loss: 28.7973\n","Epoch 27/200\n","10/10 - 0s - loss: 25.8565\n","Epoch 28/200\n","10/10 - 0s - loss: 25.1795\n","Epoch 29/200\n","10/10 - 0s - loss: 24.9320\n","Epoch 30/200\n","10/10 - 0s - loss: 26.9209\n","Epoch 31/200\n","10/10 - 0s - loss: 33.4655\n","Epoch 32/200\n","10/10 - 0s - loss: 48.0597\n","Epoch 33/200\n","10/10 - 0s - loss: 35.4553\n","Epoch 34/200\n","10/10 - 0s - loss: 38.0439\n","Epoch 35/200\n","10/10 - 0s - loss: 24.0951\n","Epoch 36/200\n","10/10 - 0s - loss: 25.1445\n","Epoch 37/200\n","10/10 - 0s - loss: 24.2092\n","Epoch 38/200\n","10/10 - 0s - loss: 18.5115\n","Epoch 39/200\n","10/10 - 0s - loss: 18.3097\n","Epoch 40/200\n","10/10 - 0s - loss: 17.0983\n","Epoch 41/200\n","10/10 - 0s - loss: 19.6515\n","Epoch 42/200\n","10/10 - 0s - loss: 17.7605\n","Epoch 43/200\n","10/10 - 0s - loss: 17.0966\n","Epoch 44/200\n","10/10 - 0s - loss: 17.5480\n","Epoch 45/200\n","10/10 - 0s - loss: 15.7430\n","Epoch 46/200\n","10/10 - 0s - loss: 19.0350\n","Epoch 47/200\n","10/10 - 0s - loss: 16.7635\n","Epoch 48/200\n","10/10 - 0s - loss: 17.2072\n","Epoch 49/200\n","10/10 - 0s - loss: 16.9487\n","Epoch 50/200\n","10/10 - 0s - loss: 14.7993\n","Epoch 51/200\n","10/10 - 0s - loss: 14.7874\n","Epoch 52/200\n","10/10 - 0s - loss: 17.5069\n","Epoch 53/200\n","10/10 - 0s - loss: 13.6214\n","Epoch 54/200\n","10/10 - 0s - loss: 14.0724\n","Epoch 55/200\n","10/10 - 0s - loss: 18.4198\n","Epoch 56/200\n","10/10 - 0s - loss: 15.9416\n","Epoch 57/200\n","10/10 - 0s - loss: 21.5673\n","Epoch 58/200\n","10/10 - 0s - loss: 22.8560\n","Epoch 59/200\n","10/10 - 0s - loss: 16.8881\n","Epoch 60/200\n","10/10 - 0s - loss: 15.9922\n","Epoch 61/200\n","10/10 - 0s - loss: 13.7514\n","Epoch 62/200\n","10/10 - 0s - loss: 22.6096\n","Epoch 63/200\n","10/10 - 0s - loss: 17.1855\n","Epoch 64/200\n","10/10 - 0s - loss: 12.8065\n","Epoch 65/200\n","10/10 - 0s - loss: 14.8528\n","Epoch 66/200\n","10/10 - 0s - loss: 14.5728\n","Epoch 67/200\n","10/10 - 0s - loss: 18.3683\n","Epoch 68/200\n","10/10 - 0s - loss: 15.5008\n","Epoch 69/200\n","10/10 - 0s - loss: 12.7352\n","Epoch 70/200\n","10/10 - 0s - loss: 13.1649\n","Epoch 71/200\n","10/10 - 0s - loss: 11.8919\n","Epoch 72/200\n","10/10 - 0s - loss: 13.1655\n","Epoch 73/200\n","10/10 - 0s - loss: 14.6156\n","Epoch 74/200\n","10/10 - 0s - loss: 17.4018\n","Epoch 75/200\n","10/10 - 0s - loss: 16.7411\n","Epoch 76/200\n","10/10 - 0s - loss: 18.0073\n","Epoch 77/200\n","10/10 - 0s - loss: 13.1195\n","Epoch 78/200\n","10/10 - 0s - loss: 12.5244\n","Epoch 79/200\n","10/10 - 0s - loss: 11.7999\n","Epoch 80/200\n","10/10 - 0s - loss: 14.5803\n","Epoch 81/200\n","10/10 - 0s - loss: 14.8649\n","Epoch 82/200\n","10/10 - 0s - loss: 15.7660\n","Epoch 83/200\n","10/10 - 0s - loss: 12.7684\n","Epoch 84/200\n","10/10 - 0s - loss: 12.0346\n","Epoch 85/200\n","10/10 - 0s - loss: 17.3559\n","Epoch 86/200\n","10/10 - 0s - loss: 20.5169\n","Epoch 87/200\n","10/10 - 0s - loss: 33.4739\n","Epoch 88/200\n","10/10 - 0s - loss: 28.2669\n","Epoch 89/200\n","10/10 - 0s - loss: 18.0699\n","Epoch 90/200\n","10/10 - 0s - loss: 19.3370\n","Epoch 91/200\n","10/10 - 0s - loss: 21.6218\n","Epoch 92/200\n","10/10 - 0s - loss: 32.0251\n","Epoch 93/200\n","10/10 - 0s - loss: 14.7299\n","Epoch 94/200\n","10/10 - 0s - loss: 16.0493\n","Epoch 95/200\n","10/10 - 0s - loss: 12.5031\n","Epoch 96/200\n","10/10 - 0s - loss: 12.0646\n","Epoch 97/200\n","10/10 - 0s - loss: 13.4305\n","Epoch 98/200\n","10/10 - 0s - loss: 13.8571\n","Epoch 99/200\n","10/10 - 0s - loss: 14.8714\n","Epoch 100/200\n","10/10 - 0s - loss: 12.2644\n","Epoch 101/200\n","10/10 - 0s - loss: 12.6252\n","Epoch 102/200\n","10/10 - 0s - loss: 12.4883\n","Epoch 103/200\n","10/10 - 0s - loss: 13.5851\n","Epoch 104/200\n","10/10 - 0s - loss: 18.0159\n","Epoch 105/200\n","10/10 - 0s - loss: 13.1630\n","Epoch 106/200\n","10/10 - 0s - loss: 13.4257\n","Epoch 107/200\n","10/10 - 0s - loss: 35.1958\n","Epoch 108/200\n","10/10 - 0s - loss: 22.9729\n","Epoch 109/200\n","10/10 - 0s - loss: 15.3886\n","Epoch 110/200\n","10/10 - 0s - loss: 16.3288\n","Epoch 111/200\n","10/10 - 0s - loss: 14.8361\n","Epoch 112/200\n","10/10 - 0s - loss: 14.9841\n","Epoch 113/200\n","10/10 - 0s - loss: 17.4380\n","Epoch 114/200\n","10/10 - 0s - loss: 18.1656\n","Epoch 115/200\n","10/10 - 0s - loss: 13.4132\n","Epoch 116/200\n","10/10 - 0s - loss: 21.2410\n","Epoch 117/200\n","10/10 - 0s - loss: 28.0525\n","Epoch 118/200\n","10/10 - 0s - loss: 11.8186\n","Epoch 119/200\n","10/10 - 0s - loss: 13.9803\n","Epoch 120/200\n","10/10 - 0s - loss: 19.5398\n","Epoch 121/200\n","10/10 - 0s - loss: 19.9432\n","Epoch 122/200\n","10/10 - 0s - loss: 16.7649\n","Epoch 123/200\n","10/10 - 0s - loss: 57.7404\n","Epoch 124/200\n","10/10 - 0s - loss: 101.2794\n","Epoch 125/200\n","10/10 - 0s - loss: 118.7633\n","Epoch 126/200\n","10/10 - 0s - loss: 100.1920\n","Epoch 127/200\n","10/10 - 0s - loss: 44.4596\n","Epoch 128/200\n","10/10 - 0s - loss: 56.1898\n","Epoch 129/200\n","10/10 - 0s - loss: 37.5396\n","Epoch 130/200\n","10/10 - 0s - loss: 36.0584\n","Epoch 131/200\n","10/10 - 0s - loss: 15.0478\n","Epoch 132/200\n","10/10 - 0s - loss: 11.3672\n","Epoch 133/200\n","10/10 - 0s - loss: 14.3116\n","Epoch 134/200\n","10/10 - 0s - loss: 14.3363\n","Epoch 135/200\n","10/10 - 0s - loss: 11.7997\n","Epoch 136/200\n","10/10 - 0s - loss: 15.5796\n","Epoch 137/200\n","10/10 - 0s - loss: 17.4984\n","Epoch 138/200\n","10/10 - 0s - loss: 19.6046\n","Epoch 139/200\n","10/10 - 0s - loss: 18.1340\n","Epoch 140/200\n","10/10 - 0s - loss: 11.9719\n","Epoch 141/200\n","10/10 - 0s - loss: 14.8214\n","Epoch 142/200\n","10/10 - 0s - loss: 11.9069\n","Epoch 143/200\n","10/10 - 0s - loss: 12.1499\n","Epoch 144/200\n","10/10 - 0s - loss: 12.6367\n","Epoch 145/200\n","10/10 - 0s - loss: 19.0378\n","Epoch 146/200\n","10/10 - 0s - loss: 26.1779\n","Epoch 147/200\n","10/10 - 0s - loss: 23.3826\n","Epoch 148/200\n","10/10 - 0s - loss: 26.4073\n","Epoch 149/200\n","10/10 - 0s - loss: 14.7753\n","Epoch 150/200\n","10/10 - 0s - loss: 13.7298\n","Epoch 151/200\n","10/10 - 0s - loss: 12.0000\n","Epoch 152/200\n","10/10 - 0s - loss: 17.6030\n","Epoch 153/200\n","10/10 - 0s - loss: 25.4841\n","Epoch 154/200\n","10/10 - 0s - loss: 34.4595\n","Epoch 155/200\n","10/10 - 0s - loss: 43.1986\n","Epoch 156/200\n","10/10 - 0s - loss: 20.5470\n","Epoch 157/200\n","10/10 - 0s - loss: 31.8869\n","Epoch 158/200\n","10/10 - 0s - loss: 40.0606\n","Epoch 159/200\n","10/10 - 0s - loss: 41.2158\n","Epoch 160/200\n","10/10 - 0s - loss: 36.8152\n","Epoch 161/200\n","10/10 - 0s - loss: 22.2807\n","Epoch 162/200\n","10/10 - 0s - loss: 16.9684\n","Epoch 163/200\n","10/10 - 0s - loss: 24.3798\n","Epoch 164/200\n","10/10 - 0s - loss: 14.7085\n","Epoch 165/200\n","10/10 - 0s - loss: 13.2297\n","Epoch 166/200\n","10/10 - 0s - loss: 14.4085\n","Epoch 167/200\n","10/10 - 0s - loss: 12.9077\n","Epoch 168/200\n","10/10 - 0s - loss: 16.5065\n","Epoch 169/200\n","10/10 - 0s - loss: 23.3319\n","Epoch 170/200\n","10/10 - 0s - loss: 18.5771\n","Epoch 171/200\n","10/10 - 0s - loss: 30.0022\n","Epoch 172/200\n","10/10 - 0s - loss: 12.9150\n","Epoch 173/200\n","10/10 - 0s - loss: 13.0735\n","Epoch 174/200\n","10/10 - 0s - loss: 10.8578\n","Epoch 175/200\n","10/10 - 0s - loss: 13.3640\n","Epoch 176/200\n","10/10 - 0s - loss: 14.9187\n","Epoch 177/200\n","10/10 - 0s - loss: 12.1718\n","Epoch 178/200\n","10/10 - 0s - loss: 11.9065\n","Epoch 179/200\n","10/10 - 0s - loss: 13.8437\n","Epoch 180/200\n","10/10 - 0s - loss: 13.2517\n","Epoch 181/200\n","10/10 - 0s - loss: 11.2606\n","Epoch 182/200\n","10/10 - 0s - loss: 15.7902\n","Epoch 183/200\n","10/10 - 0s - loss: 13.9141\n","Epoch 184/200\n","10/10 - 0s - loss: 13.3339\n","Epoch 185/200\n","10/10 - 0s - loss: 11.4969\n","Epoch 186/200\n","10/10 - 0s - loss: 15.4305\n","Epoch 187/200\n","10/10 - 0s - loss: 20.6153\n","Epoch 188/200\n","10/10 - 0s - loss: 17.6628\n","Epoch 189/200\n","10/10 - 0s - loss: 19.5121\n","Epoch 190/200\n","10/10 - 0s - loss: 15.6525\n","Epoch 191/200\n","10/10 - 0s - loss: 16.3318\n","Epoch 192/200\n","10/10 - 0s - loss: 22.6329\n","Epoch 193/200\n","10/10 - 0s - loss: 38.2369\n","Epoch 194/200\n","10/10 - 0s - loss: 21.5954\n","Epoch 195/200\n","10/10 - 0s - loss: 13.3759\n","Epoch 196/200\n","10/10 - 0s - loss: 18.4907\n","Epoch 197/200\n","10/10 - 0s - loss: 14.5160\n","Epoch 198/200\n","10/10 - 0s - loss: 15.2592\n","Epoch 199/200\n","10/10 - 0s - loss: 14.9449\n","Epoch 200/200\n","10/10 - 0s - loss: 14.3480\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fa6afa78520>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["model = Sequential()\n","model.add(Dense(1024, input_dim=X.shape[1], activation='relu')) # Hidden 1\n","model.add(Dense(1)) # Output\n","model.compile(loss='mean_squared_error', optimizer='adam')\n","model.fit(X_train,y_train,verbose=2,epochs=200)"]},{"cell_type":"markdown","metadata":{"id":"pvQ6iFdMRVUa"},"source":["This seems to be giving a much better answer, but notice that loss is jumping about.  Given that this is occurring early, it suggest the number of hidden units is too high.  So let's bring this number down."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6UqjAYESRVUa","outputId":"4c1150e3-2d52-4a06-ce29-6885764b5a92"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/200\n","10/10 - 0s - loss: 16148.8477 - 219ms/epoch - 22ms/step\n","Epoch 2/200\n","10/10 - 0s - loss: 5442.0347 - 9ms/epoch - 932us/step\n","Epoch 3/200\n","10/10 - 0s - loss: 1148.0663 - 15ms/epoch - 2ms/step\n","Epoch 4/200\n","10/10 - 0s - loss: 511.9598 - 34ms/epoch - 3ms/step\n","Epoch 5/200\n","10/10 - 0s - loss: 427.9882 - 16ms/epoch - 2ms/step\n","Epoch 6/200\n","10/10 - 0s - loss: 278.0034 - 9ms/epoch - 868us/step\n","Epoch 7/200\n","10/10 - 0s - loss: 222.4591 - 20ms/epoch - 2ms/step\n","Epoch 8/200\n","10/10 - 0s - loss: 212.5614 - 17ms/epoch - 2ms/step\n","Epoch 9/200\n","10/10 - 0s - loss: 207.6781 - 9ms/epoch - 879us/step\n","Epoch 10/200\n","10/10 - 0s - loss: 201.7340 - 19ms/epoch - 2ms/step\n","Epoch 11/200\n","10/10 - 0s - loss: 196.1636 - 22ms/epoch - 2ms/step\n","Epoch 12/200\n","10/10 - 0s - loss: 191.0546 - 15ms/epoch - 1ms/step\n","Epoch 13/200\n","10/10 - 0s - loss: 191.1834 - 14ms/epoch - 1ms/step\n","Epoch 14/200\n","10/10 - 0s - loss: 186.6448 - 21ms/epoch - 2ms/step\n","Epoch 15/200\n","10/10 - 0s - loss: 181.8088 - 25ms/epoch - 3ms/step\n","Epoch 16/200\n","10/10 - 0s - loss: 176.5059 - 24ms/epoch - 2ms/step\n","Epoch 17/200\n","10/10 - 0s - loss: 169.3181 - 16ms/epoch - 2ms/step\n","Epoch 18/200\n","10/10 - 0s - loss: 163.5608 - 16ms/epoch - 2ms/step\n","Epoch 19/200\n","10/10 - 0s - loss: 160.0112 - 20ms/epoch - 2ms/step\n","Epoch 20/200\n","10/10 - 0s - loss: 155.8512 - 13ms/epoch - 1ms/step\n","Epoch 21/200\n","10/10 - 0s - loss: 153.6981 - 21ms/epoch - 2ms/step\n","Epoch 22/200\n","10/10 - 0s - loss: 149.3314 - 12ms/epoch - 1ms/step\n","Epoch 23/200\n","10/10 - 0s - loss: 148.8721 - 27ms/epoch - 3ms/step\n","Epoch 24/200\n","10/10 - 0s - loss: 151.2934 - 15ms/epoch - 1ms/step\n","Epoch 25/200\n","10/10 - 0s - loss: 140.0208 - 33ms/epoch - 3ms/step\n","Epoch 26/200\n","10/10 - 0s - loss: 139.1142 - 14ms/epoch - 1ms/step\n","Epoch 27/200\n","10/10 - 0s - loss: 136.5331 - 19ms/epoch - 2ms/step\n","Epoch 28/200\n","10/10 - 0s - loss: 132.4260 - 13ms/epoch - 1ms/step\n","Epoch 29/200\n","10/10 - 0s - loss: 127.2538 - 32ms/epoch - 3ms/step\n","Epoch 30/200\n","10/10 - 0s - loss: 124.4528 - 13ms/epoch - 1ms/step\n","Epoch 31/200\n","10/10 - 0s - loss: 123.6127 - 31ms/epoch - 3ms/step\n","Epoch 32/200\n","10/10 - 0s - loss: 122.9249 - 13ms/epoch - 1ms/step\n","Epoch 33/200\n","10/10 - 0s - loss: 118.6430 - 27ms/epoch - 3ms/step\n","Epoch 34/200\n","10/10 - 0s - loss: 118.6302 - 13ms/epoch - 1ms/step\n","Epoch 35/200\n","10/10 - 0s - loss: 119.6031 - 13ms/epoch - 1ms/step\n","Epoch 36/200\n","10/10 - 0s - loss: 112.2249 - 18ms/epoch - 2ms/step\n","Epoch 37/200\n","10/10 - 0s - loss: 112.0397 - 11ms/epoch - 1ms/step\n","Epoch 38/200\n","10/10 - 0s - loss: 113.5650 - 22ms/epoch - 2ms/step\n","Epoch 39/200\n","10/10 - 0s - loss: 112.0308 - 13ms/epoch - 1ms/step\n","Epoch 40/200\n","10/10 - 0s - loss: 113.1394 - 15ms/epoch - 1ms/step\n","Epoch 41/200\n","10/10 - 0s - loss: 120.6482 - 11ms/epoch - 1ms/step\n","Epoch 42/200\n","10/10 - 0s - loss: 106.0746 - 17ms/epoch - 2ms/step\n","Epoch 43/200\n","10/10 - 0s - loss: 103.2856 - 12ms/epoch - 1ms/step\n","Epoch 44/200\n","10/10 - 0s - loss: 98.2128 - 12ms/epoch - 1ms/step\n","Epoch 45/200\n","10/10 - 0s - loss: 96.5726 - 14ms/epoch - 1ms/step\n","Epoch 46/200\n","10/10 - 0s - loss: 95.8454 - 24ms/epoch - 2ms/step\n","Epoch 47/200\n","10/10 - 0s - loss: 94.0792 - 14ms/epoch - 1ms/step\n","Epoch 48/200\n","10/10 - 0s - loss: 94.8284 - 12ms/epoch - 1ms/step\n","Epoch 49/200\n","10/10 - 0s - loss: 90.8584 - 15ms/epoch - 2ms/step\n","Epoch 50/200\n","10/10 - 0s - loss: 91.0732 - 12ms/epoch - 1ms/step\n","Epoch 51/200\n","10/10 - 0s - loss: 89.0957 - 34ms/epoch - 3ms/step\n","Epoch 52/200\n","10/10 - 0s - loss: 88.5450 - 26ms/epoch - 3ms/step\n","Epoch 53/200\n","10/10 - 0s - loss: 87.0763 - 16ms/epoch - 2ms/step\n","Epoch 54/200\n","10/10 - 0s - loss: 84.5711 - 16ms/epoch - 2ms/step\n","Epoch 55/200\n","10/10 - 0s - loss: 82.5928 - 17ms/epoch - 2ms/step\n","Epoch 56/200\n","10/10 - 0s - loss: 82.7401 - 20ms/epoch - 2ms/step\n","Epoch 57/200\n","10/10 - 0s - loss: 83.5732 - 31ms/epoch - 3ms/step\n","Epoch 58/200\n","10/10 - 0s - loss: 79.0532 - 25ms/epoch - 3ms/step\n","Epoch 59/200\n","10/10 - 0s - loss: 78.0939 - 12ms/epoch - 1ms/step\n","Epoch 60/200\n","10/10 - 0s - loss: 79.6911 - 30ms/epoch - 3ms/step\n","Epoch 61/200\n","10/10 - 0s - loss: 76.9097 - 18ms/epoch - 2ms/step\n","Epoch 62/200\n","10/10 - 0s - loss: 74.9991 - 20ms/epoch - 2ms/step\n","Epoch 63/200\n","10/10 - 0s - loss: 77.2197 - 20ms/epoch - 2ms/step\n","Epoch 64/200\n","10/10 - 0s - loss: 73.9141 - 22ms/epoch - 2ms/step\n","Epoch 65/200\n","10/10 - 0s - loss: 74.6058 - 17ms/epoch - 2ms/step\n","Epoch 66/200\n","10/10 - 0s - loss: 73.8334 - 13ms/epoch - 1ms/step\n","Epoch 67/200\n","10/10 - 0s - loss: 68.8623 - 16ms/epoch - 2ms/step\n","Epoch 68/200\n","10/10 - 0s - loss: 70.8893 - 20ms/epoch - 2ms/step\n","Epoch 69/200\n","10/10 - 0s - loss: 66.8193 - 14ms/epoch - 1ms/step\n","Epoch 70/200\n","10/10 - 0s - loss: 66.3544 - 13ms/epoch - 1ms/step\n","Epoch 71/200\n","10/10 - 0s - loss: 66.5424 - 13ms/epoch - 1ms/step\n","Epoch 72/200\n","10/10 - 0s - loss: 65.1300 - 16ms/epoch - 2ms/step\n","Epoch 73/200\n","10/10 - 0s - loss: 64.2631 - 16ms/epoch - 2ms/step\n","Epoch 74/200\n","10/10 - 0s - loss: 61.7491 - 10ms/epoch - 980us/step\n","Epoch 75/200\n","10/10 - 0s - loss: 60.9707 - 12ms/epoch - 1ms/step\n","Epoch 76/200\n","10/10 - 0s - loss: 65.4066 - 10ms/epoch - 1ms/step\n","Epoch 77/200\n","10/10 - 0s - loss: 70.9469 - 15ms/epoch - 1ms/step\n","Epoch 78/200\n","10/10 - 0s - loss: 61.5548 - 15ms/epoch - 1ms/step\n","Epoch 79/200\n","10/10 - 0s - loss: 57.3397 - 15ms/epoch - 2ms/step\n","Epoch 80/200\n","10/10 - 0s - loss: 57.7566 - 12ms/epoch - 1ms/step\n","Epoch 81/200\n","10/10 - 0s - loss: 56.8892 - 9ms/epoch - 859us/step\n","Epoch 82/200\n","10/10 - 0s - loss: 53.0805 - 14ms/epoch - 1ms/step\n","Epoch 83/200\n","10/10 - 0s - loss: 52.4929 - 15ms/epoch - 1ms/step\n","Epoch 84/200\n","10/10 - 0s - loss: 52.3680 - 15ms/epoch - 2ms/step\n","Epoch 85/200\n","10/10 - 0s - loss: 51.4897 - 11ms/epoch - 1ms/step\n","Epoch 86/200\n","10/10 - 0s - loss: 51.3476 - 13ms/epoch - 1ms/step\n","Epoch 87/200\n","10/10 - 0s - loss: 49.0568 - 10ms/epoch - 1ms/step\n","Epoch 88/200\n","10/10 - 0s - loss: 48.9383 - 11ms/epoch - 1ms/step\n","Epoch 89/200\n","10/10 - 0s - loss: 50.2759 - 16ms/epoch - 2ms/step\n","Epoch 90/200\n","10/10 - 0s - loss: 50.2571 - 19ms/epoch - 2ms/step\n","Epoch 91/200\n","10/10 - 0s - loss: 45.6212 - 17ms/epoch - 2ms/step\n","Epoch 92/200\n","10/10 - 0s - loss: 45.3231 - 15ms/epoch - 1ms/step\n","Epoch 93/200\n","10/10 - 0s - loss: 44.3253 - 16ms/epoch - 2ms/step\n","Epoch 94/200\n","10/10 - 0s - loss: 43.0487 - 13ms/epoch - 1ms/step\n","Epoch 95/200\n","10/10 - 0s - loss: 46.3781 - 17ms/epoch - 2ms/step\n","Epoch 96/200\n","10/10 - 0s - loss: 42.3763 - 18ms/epoch - 2ms/step\n","Epoch 97/200\n","10/10 - 0s - loss: 41.9165 - 10ms/epoch - 1ms/step\n","Epoch 98/200\n","10/10 - 0s - loss: 42.5544 - 15ms/epoch - 2ms/step\n","Epoch 99/200\n","10/10 - 0s - loss: 41.2189 - 16ms/epoch - 2ms/step\n","Epoch 100/200\n","10/10 - 0s - loss: 39.3172 - 9ms/epoch - 877us/step\n","Epoch 101/200\n","10/10 - 0s - loss: 40.9963 - 13ms/epoch - 1ms/step\n","Epoch 102/200\n","10/10 - 0s - loss: 43.3893 - 13ms/epoch - 1ms/step\n","Epoch 103/200\n","10/10 - 0s - loss: 38.0605 - 11ms/epoch - 1ms/step\n","Epoch 104/200\n","10/10 - 0s - loss: 36.5584 - 9ms/epoch - 880us/step\n","Epoch 105/200\n","10/10 - 0s - loss: 35.9468 - 9ms/epoch - 889us/step\n","Epoch 106/200\n","10/10 - 0s - loss: 34.8243 - 15ms/epoch - 1ms/step\n","Epoch 107/200\n","10/10 - 0s - loss: 37.5122 - 11ms/epoch - 1ms/step\n","Epoch 108/200\n","10/10 - 0s - loss: 34.2882 - 10ms/epoch - 995us/step\n","Epoch 109/200\n","10/10 - 0s - loss: 32.8786 - 11ms/epoch - 1ms/step\n","Epoch 110/200\n","10/10 - 0s - loss: 33.1371 - 14ms/epoch - 1ms/step\n","Epoch 111/200\n","10/10 - 0s - loss: 32.9716 - 14ms/epoch - 1ms/step\n","Epoch 112/200\n","10/10 - 0s - loss: 35.1402 - 13ms/epoch - 1ms/step\n","Epoch 113/200\n","10/10 - 0s - loss: 30.8680 - 16ms/epoch - 2ms/step\n","Epoch 114/200\n","10/10 - 0s - loss: 30.4482 - 14ms/epoch - 1ms/step\n","Epoch 115/200\n","10/10 - 0s - loss: 29.5825 - 15ms/epoch - 2ms/step\n","Epoch 116/200\n","10/10 - 0s - loss: 30.1090 - 15ms/epoch - 2ms/step\n","Epoch 117/200\n","10/10 - 0s - loss: 29.3006 - 18ms/epoch - 2ms/step\n","Epoch 118/200\n","10/10 - 0s - loss: 28.3699 - 16ms/epoch - 2ms/step\n","Epoch 119/200\n","10/10 - 0s - loss: 27.7176 - 16ms/epoch - 2ms/step\n","Epoch 120/200\n","10/10 - 0s - loss: 27.4504 - 15ms/epoch - 1ms/step\n","Epoch 121/200\n","10/10 - 0s - loss: 26.9856 - 20ms/epoch - 2ms/step\n","Epoch 122/200\n","10/10 - 0s - loss: 28.5420 - 18ms/epoch - 2ms/step\n","Epoch 123/200\n","10/10 - 0s - loss: 28.7595 - 39ms/epoch - 4ms/step\n","Epoch 124/200\n","10/10 - 0s - loss: 28.2572 - 28ms/epoch - 3ms/step\n","Epoch 125/200\n","10/10 - 0s - loss: 26.2367 - 37ms/epoch - 4ms/step\n","Epoch 126/200\n","10/10 - 0s - loss: 24.2585 - 14ms/epoch - 1ms/step\n","Epoch 127/200\n","10/10 - 0s - loss: 24.0226 - 13ms/epoch - 1ms/step\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 128/200\n","10/10 - 0s - loss: 24.1571 - 18ms/epoch - 2ms/step\n","Epoch 129/200\n","10/10 - 0s - loss: 23.7788 - 10ms/epoch - 976us/step\n","Epoch 130/200\n","10/10 - 0s - loss: 22.6007 - 13ms/epoch - 1ms/step\n","Epoch 131/200\n","10/10 - 0s - loss: 22.3085 - 9ms/epoch - 861us/step\n","Epoch 132/200\n","10/10 - 0s - loss: 23.3387 - 10ms/epoch - 1ms/step\n","Epoch 133/200\n","10/10 - 0s - loss: 22.3783 - 13ms/epoch - 1ms/step\n","Epoch 134/200\n","10/10 - 0s - loss: 24.2633 - 11ms/epoch - 1ms/step\n","Epoch 135/200\n","10/10 - 0s - loss: 24.9025 - 8ms/epoch - 802us/step\n","Epoch 136/200\n","10/10 - 0s - loss: 25.1157 - 9ms/epoch - 866us/step\n","Epoch 137/200\n","10/10 - 0s - loss: 24.6884 - 9ms/epoch - 901us/step\n","Epoch 138/200\n","10/10 - 0s - loss: 24.5831 - 10ms/epoch - 1ms/step\n","Epoch 139/200\n","10/10 - 0s - loss: 20.9741 - 9ms/epoch - 928us/step\n","Epoch 140/200\n","10/10 - 0s - loss: 19.4780 - 10ms/epoch - 981us/step\n","Epoch 141/200\n","10/10 - 0s - loss: 22.3112 - 13ms/epoch - 1ms/step\n","Epoch 142/200\n","10/10 - 0s - loss: 20.0635 - 12ms/epoch - 1ms/step\n","Epoch 143/200\n","10/10 - 0s - loss: 19.2462 - 12ms/epoch - 1ms/step\n","Epoch 144/200\n","10/10 - 0s - loss: 18.8329 - 13ms/epoch - 1ms/step\n","Epoch 145/200\n","10/10 - 0s - loss: 18.4845 - 10ms/epoch - 1ms/step\n","Epoch 146/200\n","10/10 - 0s - loss: 18.3139 - 11ms/epoch - 1ms/step\n","Epoch 147/200\n","10/10 - 0s - loss: 18.4884 - 13ms/epoch - 1ms/step\n","Epoch 148/200\n","10/10 - 0s - loss: 18.2027 - 12ms/epoch - 1ms/step\n","Epoch 149/200\n","10/10 - 0s - loss: 18.0863 - 13ms/epoch - 1ms/step\n","Epoch 150/200\n","10/10 - 0s - loss: 18.1795 - 9ms/epoch - 950us/step\n","Epoch 151/200\n","10/10 - 0s - loss: 18.1077 - 9ms/epoch - 926us/step\n","Epoch 152/200\n","10/10 - 0s - loss: 17.4915 - 13ms/epoch - 1ms/step\n","Epoch 153/200\n","10/10 - 0s - loss: 19.9388 - 9ms/epoch - 938us/step\n","Epoch 154/200\n","10/10 - 0s - loss: 19.7505 - 12ms/epoch - 1ms/step\n","Epoch 155/200\n","10/10 - 0s - loss: 18.4126 - 10ms/epoch - 966us/step\n","Epoch 156/200\n","10/10 - 0s - loss: 17.3303 - 13ms/epoch - 1ms/step\n","Epoch 157/200\n","10/10 - 0s - loss: 17.1586 - 10ms/epoch - 1ms/step\n","Epoch 158/200\n","10/10 - 0s - loss: 15.8615 - 10ms/epoch - 1ms/step\n","Epoch 159/200\n","10/10 - 0s - loss: 16.2886 - 12ms/epoch - 1ms/step\n","Epoch 160/200\n","10/10 - 0s - loss: 16.0169 - 12ms/epoch - 1ms/step\n","Epoch 161/200\n","10/10 - 0s - loss: 18.8069 - 13ms/epoch - 1ms/step\n","Epoch 162/200\n","10/10 - 0s - loss: 19.0457 - 13ms/epoch - 1ms/step\n","Epoch 163/200\n","10/10 - 0s - loss: 18.7052 - 14ms/epoch - 1ms/step\n","Epoch 164/200\n","10/10 - 0s - loss: 21.1225 - 12ms/epoch - 1ms/step\n","Epoch 165/200\n","10/10 - 0s - loss: 21.1001 - 16ms/epoch - 2ms/step\n","Epoch 166/200\n","10/10 - 0s - loss: 23.6604 - 15ms/epoch - 1ms/step\n","Epoch 167/200\n","10/10 - 0s - loss: 18.1347 - 16ms/epoch - 2ms/step\n","Epoch 168/200\n","10/10 - 0s - loss: 16.5604 - 39ms/epoch - 4ms/step\n","Epoch 169/200\n","10/10 - 0s - loss: 14.9307 - 24ms/epoch - 2ms/step\n","Epoch 170/200\n","10/10 - 0s - loss: 14.7546 - 16ms/epoch - 2ms/step\n","Epoch 171/200\n","10/10 - 0s - loss: 17.5498 - 32ms/epoch - 3ms/step\n","Epoch 172/200\n","10/10 - 0s - loss: 16.6725 - 29ms/epoch - 3ms/step\n","Epoch 173/200\n","10/10 - 0s - loss: 15.1919 - 100ms/epoch - 10ms/step\n","Epoch 174/200\n","10/10 - 0s - loss: 16.8887 - 33ms/epoch - 3ms/step\n","Epoch 175/200\n","10/10 - 0s - loss: 16.7804 - 32ms/epoch - 3ms/step\n","Epoch 176/200\n","10/10 - 0s - loss: 13.7765 - 18ms/epoch - 2ms/step\n","Epoch 177/200\n","10/10 - 0s - loss: 14.4965 - 21ms/epoch - 2ms/step\n","Epoch 178/200\n","10/10 - 0s - loss: 13.9526 - 29ms/epoch - 3ms/step\n","Epoch 179/200\n","10/10 - 0s - loss: 14.7222 - 18ms/epoch - 2ms/step\n","Epoch 180/200\n","10/10 - 0s - loss: 13.5888 - 20ms/epoch - 2ms/step\n","Epoch 181/200\n","10/10 - 0s - loss: 14.3199 - 12ms/epoch - 1ms/step\n","Epoch 182/200\n","10/10 - 0s - loss: 13.3854 - 14ms/epoch - 1ms/step\n","Epoch 183/200\n","10/10 - 0s - loss: 14.5976 - 15ms/epoch - 2ms/step\n","Epoch 184/200\n","10/10 - 0s - loss: 13.6757 - 14ms/epoch - 1ms/step\n","Epoch 185/200\n","10/10 - 0s - loss: 13.3040 - 14ms/epoch - 1ms/step\n","Epoch 186/200\n","10/10 - 0s - loss: 13.6482 - 12ms/epoch - 1ms/step\n","Epoch 187/200\n","10/10 - 0s - loss: 13.9712 - 19ms/epoch - 2ms/step\n","Epoch 188/200\n","10/10 - 0s - loss: 13.0177 - 11ms/epoch - 1ms/step\n","Epoch 189/200\n","10/10 - 0s - loss: 14.2146 - 16ms/epoch - 2ms/step\n","Epoch 190/200\n","10/10 - 0s - loss: 14.8334 - 19ms/epoch - 2ms/step\n","Epoch 191/200\n","10/10 - 0s - loss: 14.6297 - 7ms/epoch - 725us/step\n","Epoch 192/200\n","10/10 - 0s - loss: 13.1576 - 12ms/epoch - 1ms/step\n","Epoch 193/200\n","10/10 - 0s - loss: 12.8505 - 11ms/epoch - 1ms/step\n","Epoch 194/200\n","10/10 - 0s - loss: 13.6541 - 16ms/epoch - 2ms/step\n","Epoch 195/200\n","10/10 - 0s - loss: 13.4668 - 14ms/epoch - 1ms/step\n","Epoch 196/200\n","10/10 - 0s - loss: 14.8412 - 14ms/epoch - 1ms/step\n","Epoch 197/200\n","10/10 - 0s - loss: 13.5377 - 16ms/epoch - 2ms/step\n","Epoch 198/200\n","10/10 - 0s - loss: 13.3262 - 24ms/epoch - 2ms/step\n","Epoch 199/200\n","10/10 - 0s - loss: 13.4471 - 24ms/epoch - 2ms/step\n","Epoch 200/200\n","10/10 - 0s - loss: 13.1785 - 9ms/epoch - 903us/step\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f96a35b6ac0>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["model = Sequential()\n","model.add(Dense(64, input_dim=X.shape[1], activation='relu')) # Hidden 1\n","model.add(Dense(1)) # Output\n","model.compile(loss='mean_squared_error', optimizer='adam')\n","model.fit(X_train,y_train,verbose=2,epochs=200)"]},{"cell_type":"markdown","metadata":{"id":"b7jbaPj3RVUa"},"source":["Here the output looks stable, and this might be a good answer (it's comparable to the linear regression answer we computed in Ex3).  You can experiment some more (note that you won't always get the same answer owing to the randomised initial weight values)."]},{"cell_type":"markdown","metadata":{"id":"wfONvQzbRVUb"},"source":["### Regression prediction\n","\n","Next we will perform actual predictions.  These predictions are assigned to the **pred** variable. These are all MPG predictions from the neural network.  Notice that this is a 2D array?  You can always see the dimensions of what is returned by printing out **pred.shape**.  Neural networks can return multiple values, so the result is always an array.  Here the neural network only returns 1 value per prediction (there are 398 cars, so 398 predictions).  However, a 2D array is needed because the neural network has the potential of returning more than one value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uYj8Op00RVUb","outputId":"54231443-78b6-4a23-f62b-21b11c2c3d46"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape: (100, 1)\n","[[ 9.716917 ]\n"," [22.38632  ]\n"," [ 8.3450775]\n"," [19.68946  ]\n"," [14.825657 ]\n"," [30.287333 ]\n"," [31.809189 ]\n"," [21.804083 ]\n"," [11.960735 ]\n"," [22.065765 ]]\n"]}],"source":["pred = model.predict(X_test)\n","print(\"Shape: {}\".format(pred.shape))\n","print(pred[:10])"]},{"cell_type":"markdown","metadata":{"id":"iM7rnKxPRVUb"},"source":["We would like to see how good these predictions are.  We know what the correct MPG is for each car, so we can measure how close the neural network was."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxmMkB-fRVUb","outputId":"958f443d-a3f8-45aa-ae6f-dab233bd1853"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final score (RMSE): 4.723357200622559\n"]}],"source":["# Measure RMSE error.  RMSE is common for regression.\n","score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n","print(f\"Final score (RMSE): {score}\")"]},{"cell_type":"markdown","metadata":{"id":"KEhr4zd6RVUc"},"source":["This means that, on average the predictions were within +/- 4.7 values of the correct value.  This is not great, but we will soon see how to improve it.\n","\n","We can also print out the first 10 cars, with predictions and actual MPG."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IvJes0CkRVUc","outputId":"395325b3-f5e6-4d45-9758-b05c6bbb212f"},"outputs":[{"name":"stdout","output_type":"stream","text":["1. Car name: chevrolet chevelle malibu, MPG: [18.], predicted MPG: [9.716917]\n","2. Car name: buick skylark 320, MPG: [15.], predicted MPG: [22.38632]\n","3. Car name: plymouth satellite, MPG: [18.], predicted MPG: [8.3450775]\n","4. Car name: amc rebel sst, MPG: [16.], predicted MPG: [19.68946]\n","5. Car name: ford torino, MPG: [17.], predicted MPG: [14.825657]\n","6. Car name: ford galaxie 500, MPG: [15.], predicted MPG: [30.287333]\n","7. Car name: chevrolet impala, MPG: [14.], predicted MPG: [31.809189]\n","8. Car name: plymouth fury iii, MPG: [14.], predicted MPG: [21.804083]\n","9. Car name: pontiac catalina, MPG: [14.], predicted MPG: [11.960735]\n","10. Car name: amc ambassador dpl, MPG: [15.], predicted MPG: [22.065765]\n"]}],"source":["# Sample predictions\n","for i in range(10):\n","    print(f\"{i+1}. Car name: {cars[i]}, MPG: {y[i]}, predicted MPG: {pred[i]}\")"]},{"cell_type":"markdown","metadata":{"id":"HCSfNnB7RVUc"},"source":["At this point we might consider manipulating the data.  Here, outliers, datapoint lying more than 2 standard deviations from the mean, are removed.  Then the sklearn **standard scaler** is used.\n","\n","Scaling is important with neural networks, which work particularly well when the data is normally distributed.  The standard scaler transforms the data so that it is normally distributed, and should allow the network to fit the data better.  Remember to transform your testing data too!"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"T95CWrYsRVUd","outputId":"aa866348-e781-475b-804b-1ba60baadbec"},"outputs":[{"name":"stdout","output_type":"stream","text":["Length before MPG outliers dropped: 398\n","Length after MPG outliers dropped: 388\n"]}],"source":["print(\"Length before MPG outliers dropped: {}\".format(len(df)))\n","remove_outliers(df,'mpg',2) #method call to method defined above\n","print(\"Length after MPG outliers dropped: {}\".format(len(df)))\n","\n","X,y = to_xy(df,\"mpg\")\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","sc = StandardScaler()\n","sc.fit(X_train)\n","X_train= sc.transform(X_train)\n","X_test = sc.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"9yU79pc4RVUd"},"source":["Finally, we will add a second hidden layer to the network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3sDRC17RVUd","outputId":"ba26555a-1cc5-40c4-dd62-b4099cbccfce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/250\n","10/10 - 0s - loss: 570.8696\n","Epoch 2/250\n","10/10 - 0s - loss: 534.3590\n","Epoch 3/250\n","10/10 - 0s - loss: 494.1058\n","Epoch 4/250\n","10/10 - 0s - loss: 444.2019\n","Epoch 5/250\n","10/10 - 0s - loss: 380.2871\n","Epoch 6/250\n","10/10 - 0s - loss: 308.1571\n","Epoch 7/250\n","10/10 - 0s - loss: 230.4095\n","Epoch 8/250\n","10/10 - 0s - loss: 154.8958\n","Epoch 9/250\n","10/10 - 0s - loss: 96.0061\n","Epoch 10/250\n","10/10 - 0s - loss: 58.9855\n","Epoch 11/250\n","10/10 - 0s - loss: 44.1212\n","Epoch 12/250\n","10/10 - 0s - loss: 38.8894\n","Epoch 13/250\n","10/10 - 0s - loss: 34.7113\n","Epoch 14/250\n","10/10 - 0s - loss: 30.3636\n","Epoch 15/250\n","10/10 - 0s - loss: 26.7741\n","Epoch 16/250\n","10/10 - 0s - loss: 24.0604\n","Epoch 17/250\n","10/10 - 0s - loss: 21.7730\n","Epoch 18/250\n","10/10 - 0s - loss: 19.9429\n","Epoch 19/250\n","10/10 - 0s - loss: 18.4404\n","Epoch 20/250\n","10/10 - 0s - loss: 17.2270\n","Epoch 21/250\n","10/10 - 0s - loss: 16.2454\n","Epoch 22/250\n","10/10 - 0s - loss: 15.4319\n","Epoch 23/250\n","10/10 - 0s - loss: 14.6714\n","Epoch 24/250\n","10/10 - 0s - loss: 14.0600\n","Epoch 25/250\n","10/10 - 0s - loss: 13.5625\n","Epoch 26/250\n","10/10 - 0s - loss: 13.1110\n","Epoch 27/250\n","10/10 - 0s - loss: 12.6923\n","Epoch 28/250\n","10/10 - 0s - loss: 12.4424\n","Epoch 29/250\n","10/10 - 0s - loss: 12.1189\n","Epoch 30/250\n","10/10 - 0s - loss: 11.7981\n","Epoch 31/250\n","10/10 - 0s - loss: 11.5385\n","Epoch 32/250\n","10/10 - 0s - loss: 11.3619\n","Epoch 33/250\n","10/10 - 0s - loss: 11.2105\n","Epoch 34/250\n","10/10 - 0s - loss: 10.8789\n","Epoch 35/250\n","10/10 - 0s - loss: 10.7453\n","Epoch 36/250\n","10/10 - 0s - loss: 10.5064\n","Epoch 37/250\n","10/10 - 0s - loss: 10.4526\n","Epoch 38/250\n","10/10 - 0s - loss: 10.2470\n","Epoch 39/250\n","10/10 - 0s - loss: 10.0638\n","Epoch 40/250\n","10/10 - 0s - loss: 9.9228\n","Epoch 41/250\n","10/10 - 0s - loss: 9.7953\n","Epoch 42/250\n","10/10 - 0s - loss: 9.6856\n","Epoch 43/250\n","10/10 - 0s - loss: 9.5476\n","Epoch 44/250\n","10/10 - 0s - loss: 9.4336\n","Epoch 45/250\n","10/10 - 0s - loss: 9.3329\n","Epoch 46/250\n","10/10 - 0s - loss: 9.1679\n","Epoch 47/250\n","10/10 - 0s - loss: 9.0740\n","Epoch 48/250\n","10/10 - 0s - loss: 8.9674\n","Epoch 49/250\n","10/10 - 0s - loss: 8.9263\n","Epoch 50/250\n","10/10 - 0s - loss: 8.8623\n","Epoch 51/250\n","10/10 - 0s - loss: 8.7641\n","Epoch 52/250\n","10/10 - 0s - loss: 8.6278\n","Epoch 53/250\n","10/10 - 0s - loss: 8.5392\n","Epoch 54/250\n","10/10 - 0s - loss: 8.4674\n","Epoch 55/250\n","10/10 - 0s - loss: 8.4054\n","Epoch 56/250\n","10/10 - 0s - loss: 8.3351\n","Epoch 57/250\n","10/10 - 0s - loss: 8.2349\n","Epoch 58/250\n","10/10 - 0s - loss: 8.2616\n","Epoch 59/250\n","10/10 - 0s - loss: 8.2251\n","Epoch 60/250\n","10/10 - 0s - loss: 8.0546\n","Epoch 61/250\n","10/10 - 0s - loss: 7.9593\n","Epoch 62/250\n","10/10 - 0s - loss: 7.9057\n","Epoch 63/250\n","10/10 - 0s - loss: 7.8472\n","Epoch 64/250\n","10/10 - 0s - loss: 7.7752\n","Epoch 65/250\n","10/10 - 0s - loss: 7.7462\n","Epoch 66/250\n","10/10 - 0s - loss: 7.6369\n","Epoch 67/250\n","10/10 - 0s - loss: 7.5790\n","Epoch 68/250\n","10/10 - 0s - loss: 7.5603\n","Epoch 69/250\n","10/10 - 0s - loss: 7.5005\n","Epoch 70/250\n","10/10 - 0s - loss: 7.4857\n","Epoch 71/250\n","10/10 - 0s - loss: 7.3433\n","Epoch 72/250\n","10/10 - 0s - loss: 7.2899\n","Epoch 73/250\n","10/10 - 0s - loss: 7.2485\n","Epoch 74/250\n","10/10 - 0s - loss: 7.1865\n","Epoch 75/250\n","10/10 - 0s - loss: 7.1869\n","Epoch 76/250\n","10/10 - 0s - loss: 7.1235\n","Epoch 77/250\n","10/10 - 0s - loss: 7.0601\n","Epoch 78/250\n","10/10 - 0s - loss: 7.0621\n","Epoch 79/250\n","10/10 - 0s - loss: 6.9241\n","Epoch 80/250\n","10/10 - 0s - loss: 6.9260\n","Epoch 81/250\n","10/10 - 0s - loss: 6.9197\n","Epoch 82/250\n","10/10 - 0s - loss: 6.8935\n","Epoch 83/250\n","10/10 - 0s - loss: 6.8191\n","Epoch 84/250\n","10/10 - 0s - loss: 6.7493\n","Epoch 85/250\n","10/10 - 0s - loss: 6.7558\n","Epoch 86/250\n","10/10 - 0s - loss: 6.6984\n","Epoch 87/250\n","10/10 - 0s - loss: 6.6856\n","Epoch 88/250\n","10/10 - 0s - loss: 6.6187\n","Epoch 89/250\n","10/10 - 0s - loss: 6.5252\n","Epoch 90/250\n","10/10 - 0s - loss: 6.6156\n","Epoch 91/250\n","10/10 - 0s - loss: 6.5511\n","Epoch 92/250\n","10/10 - 0s - loss: 6.4581\n","Epoch 93/250\n","10/10 - 0s - loss: 6.9458\n","Epoch 94/250\n","10/10 - 0s - loss: 6.5513\n","Epoch 95/250\n","10/10 - 0s - loss: 6.4189\n","Epoch 96/250\n","10/10 - 0s - loss: 6.4291\n","Epoch 97/250\n","10/10 - 0s - loss: 6.2946\n","Epoch 98/250\n","10/10 - 0s - loss: 6.2368\n","Epoch 99/250\n","10/10 - 0s - loss: 6.2142\n","Epoch 100/250\n","10/10 - 0s - loss: 6.1703\n","Epoch 101/250\n","10/10 - 0s - loss: 6.1017\n","Epoch 102/250\n","10/10 - 0s - loss: 6.1069\n","Epoch 103/250\n","10/10 - 0s - loss: 6.0534\n","Epoch 104/250\n","10/10 - 0s - loss: 6.1081\n","Epoch 105/250\n","10/10 - 0s - loss: 6.4384\n","Epoch 106/250\n","10/10 - 0s - loss: 5.9889\n","Epoch 107/250\n","10/10 - 0s - loss: 6.1988\n","Epoch 108/250\n","10/10 - 0s - loss: 5.9492\n","Epoch 109/250\n","10/10 - 0s - loss: 5.9060\n","Epoch 110/250\n","10/10 - 0s - loss: 5.8988\n","Epoch 111/250\n","10/10 - 0s - loss: 5.8522\n","Epoch 112/250\n","10/10 - 0s - loss: 5.7826\n","Epoch 113/250\n","10/10 - 0s - loss: 5.7658\n","Epoch 114/250\n","10/10 - 0s - loss: 5.7477\n","Epoch 115/250\n","10/10 - 0s - loss: 5.7237\n","Epoch 116/250\n","10/10 - 0s - loss: 5.6761\n","Epoch 117/250\n","10/10 - 0s - loss: 5.7920\n","Epoch 118/250\n","10/10 - 0s - loss: 5.7567\n","Epoch 119/250\n","10/10 - 0s - loss: 5.6654\n","Epoch 120/250\n","10/10 - 0s - loss: 5.7521\n","Epoch 121/250\n","10/10 - 0s - loss: 5.5394\n","Epoch 122/250\n","10/10 - 0s - loss: 5.6153\n","Epoch 123/250\n","10/10 - 0s - loss: 5.5445\n","Epoch 124/250\n","10/10 - 0s - loss: 5.5326\n","Epoch 125/250\n","10/10 - 0s - loss: 5.5006\n","Epoch 126/250\n","10/10 - 0s - loss: 5.4750\n","Epoch 127/250\n","10/10 - 0s - loss: 5.4535\n","Epoch 128/250\n","10/10 - 0s - loss: 5.4359\n","Epoch 129/250\n","10/10 - 0s - loss: 5.4336\n","Epoch 130/250\n","10/10 - 0s - loss: 5.4148\n","Epoch 131/250\n","10/10 - 0s - loss: 5.3854\n","Epoch 132/250\n","10/10 - 0s - loss: 5.4128\n","Epoch 133/250\n","10/10 - 0s - loss: 5.3490\n","Epoch 134/250\n","10/10 - 0s - loss: 5.3570\n","Epoch 135/250\n","10/10 - 0s - loss: 5.3538\n","Epoch 136/250\n","10/10 - 0s - loss: 5.3081\n","Epoch 137/250\n","10/10 - 0s - loss: 5.3164\n","Epoch 138/250\n","10/10 - 0s - loss: 5.2600\n","Epoch 139/250\n","10/10 - 0s - loss: 5.3004\n","Epoch 140/250\n","10/10 - 0s - loss: 5.3119\n","Epoch 141/250\n","10/10 - 0s - loss: 5.2653\n","Epoch 142/250\n","10/10 - 0s - loss: 5.2578\n","Epoch 143/250\n","10/10 - 0s - loss: 5.2512\n","Epoch 144/250\n","10/10 - 0s - loss: 5.2732\n","Epoch 145/250\n","10/10 - 0s - loss: 5.2842\n","Epoch 146/250\n","10/10 - 0s - loss: 5.2788\n","Epoch 147/250\n","10/10 - 0s - loss: 5.2368\n","Epoch 148/250\n","10/10 - 0s - loss: 5.1458\n","Epoch 149/250\n","10/10 - 0s - loss: 5.2207\n","Epoch 150/250\n","10/10 - 0s - loss: 5.1999\n","Epoch 151/250\n","10/10 - 0s - loss: 5.1355\n","Epoch 152/250\n","10/10 - 0s - loss: 5.0690\n","Epoch 153/250\n","10/10 - 0s - loss: 5.0568\n","Epoch 154/250\n","10/10 - 0s - loss: 5.0065\n","Epoch 155/250\n","10/10 - 0s - loss: 5.0355\n","Epoch 156/250\n","10/10 - 0s - loss: 5.0296\n","Epoch 157/250\n","10/10 - 0s - loss: 5.1194\n","Epoch 158/250\n","10/10 - 0s - loss: 5.0669\n","Epoch 159/250\n","10/10 - 0s - loss: 5.0060\n","Epoch 160/250\n","10/10 - 0s - loss: 4.9440\n","Epoch 161/250\n","10/10 - 0s - loss: 4.9487\n","Epoch 162/250\n","10/10 - 0s - loss: 4.9887\n","Epoch 163/250\n","10/10 - 0s - loss: 4.9353\n","Epoch 164/250\n","10/10 - 0s - loss: 5.0241\n","Epoch 165/250\n","10/10 - 0s - loss: 4.8895\n","Epoch 166/250\n","10/10 - 0s - loss: 4.9741\n","Epoch 167/250\n","10/10 - 0s - loss: 5.3147\n","Epoch 168/250\n","10/10 - 0s - loss: 4.9910\n","Epoch 169/250\n","10/10 - 0s - loss: 5.0681\n","Epoch 170/250\n","10/10 - 0s - loss: 4.8568\n","Epoch 171/250\n","10/10 - 0s - loss: 4.8522\n","Epoch 172/250\n","10/10 - 0s - loss: 4.8502\n","Epoch 173/250\n","10/10 - 0s - loss: 4.8420\n","Epoch 174/250\n","10/10 - 0s - loss: 4.9092\n","Epoch 175/250\n","10/10 - 0s - loss: 4.7658\n","Epoch 176/250\n","10/10 - 0s - loss: 4.7694\n","Epoch 177/250\n","10/10 - 0s - loss: 4.8045\n","Epoch 178/250\n","10/10 - 0s - loss: 4.7098\n","Epoch 179/250\n","10/10 - 0s - loss: 4.8091\n","Epoch 180/250\n","10/10 - 0s - loss: 4.8607\n","Epoch 181/250\n","10/10 - 0s - loss: 5.0849\n","Epoch 182/250\n","10/10 - 0s - loss: 4.8430\n","Epoch 183/250\n","10/10 - 0s - loss: 4.8135\n","Epoch 184/250\n","10/10 - 0s - loss: 5.0013\n","Epoch 185/250\n","10/10 - 0s - loss: 4.7515\n","Epoch 186/250\n","10/10 - 0s - loss: 4.6865\n","Epoch 187/250\n","10/10 - 0s - loss: 4.9709\n","Epoch 188/250\n","10/10 - 0s - loss: 4.6454\n","Epoch 189/250\n","10/10 - 0s - loss: 4.8552\n","Epoch 190/250\n","10/10 - 0s - loss: 4.6535\n","Epoch 191/250\n","10/10 - 0s - loss: 4.6458\n","Epoch 192/250\n","10/10 - 0s - loss: 4.8710\n","Epoch 193/250\n","10/10 - 0s - loss: 4.7887\n","Epoch 194/250\n","10/10 - 0s - loss: 4.7258\n","Epoch 195/250\n","10/10 - 0s - loss: 4.6225\n","Epoch 196/250\n","10/10 - 0s - loss: 4.7683\n","Epoch 197/250\n","10/10 - 0s - loss: 4.7055\n","Epoch 198/250\n","10/10 - 0s - loss: 4.6266\n","Epoch 199/250\n","10/10 - 0s - loss: 4.5984\n","Epoch 200/250\n","10/10 - 0s - loss: 4.5266\n","Epoch 201/250\n","10/10 - 0s - loss: 4.4981\n","Epoch 202/250\n","10/10 - 0s - loss: 4.4934\n","Epoch 203/250\n","10/10 - 0s - loss: 4.4885\n","Epoch 204/250\n","10/10 - 0s - loss: 4.4862\n","Epoch 205/250\n","10/10 - 0s - loss: 4.6820\n","Epoch 206/250\n","10/10 - 0s - loss: 4.6412\n","Epoch 207/250\n"]},{"name":"stdout","output_type":"stream","text":["10/10 - 0s - loss: 4.5443\n","Epoch 208/250\n","10/10 - 0s - loss: 4.4894\n","Epoch 209/250\n","10/10 - 0s - loss: 4.4797\n","Epoch 210/250\n","10/10 - 0s - loss: 4.4595\n","Epoch 211/250\n","10/10 - 0s - loss: 4.4036\n","Epoch 212/250\n","10/10 - 0s - loss: 4.4106\n","Epoch 213/250\n","10/10 - 0s - loss: 4.3894\n","Epoch 214/250\n","10/10 - 0s - loss: 4.3949\n","Epoch 215/250\n","10/10 - 0s - loss: 4.4799\n","Epoch 216/250\n","10/10 - 0s - loss: 4.3818\n","Epoch 217/250\n","10/10 - 0s - loss: 4.4536\n","Epoch 218/250\n","10/10 - 0s - loss: 4.3345\n","Epoch 219/250\n","10/10 - 0s - loss: 4.3487\n","Epoch 220/250\n","10/10 - 0s - loss: 4.3818\n","Epoch 221/250\n","10/10 - 0s - loss: 4.3434\n","Epoch 222/250\n","10/10 - 0s - loss: 4.4374\n","Epoch 223/250\n","10/10 - 0s - loss: 4.3492\n","Epoch 224/250\n","10/10 - 0s - loss: 4.4779\n","Epoch 225/250\n","10/10 - 0s - loss: 4.3866\n","Epoch 226/250\n","10/10 - 0s - loss: 4.3478\n","Epoch 227/250\n","10/10 - 0s - loss: 4.3154\n","Epoch 228/250\n","10/10 - 0s - loss: 4.2912\n","Epoch 229/250\n","10/10 - 0s - loss: 4.2736\n","Epoch 230/250\n","10/10 - 0s - loss: 4.2598\n","Epoch 231/250\n","10/10 - 0s - loss: 4.2568\n","Epoch 232/250\n","10/10 - 0s - loss: 4.2989\n","Epoch 233/250\n","10/10 - 0s - loss: 4.2556\n","Epoch 234/250\n","10/10 - 0s - loss: 4.3126\n","Epoch 235/250\n","10/10 - 0s - loss: 4.2544\n","Epoch 236/250\n","10/10 - 0s - loss: 4.2944\n","Epoch 237/250\n","10/10 - 0s - loss: 4.2391\n","Epoch 238/250\n","10/10 - 0s - loss: 4.2209\n","Epoch 239/250\n","10/10 - 0s - loss: 4.3062\n","Epoch 240/250\n","10/10 - 0s - loss: 4.3103\n","Epoch 241/250\n","10/10 - 0s - loss: 4.3025\n","Epoch 242/250\n","10/10 - 0s - loss: 4.2360\n","Epoch 243/250\n","10/10 - 0s - loss: 4.3307\n","Epoch 244/250\n","10/10 - 0s - loss: 4.2882\n","Epoch 245/250\n","10/10 - 0s - loss: 4.1693\n","Epoch 246/250\n","10/10 - 0s - loss: 4.2049\n","Epoch 247/250\n","10/10 - 0s - loss: 4.1737\n","Epoch 248/250\n","10/10 - 0s - loss: 4.1602\n","Epoch 249/250\n","10/10 - 0s - loss: 4.1732\n","Epoch 250/250\n","10/10 - 0s - loss: 4.2889\n","Final score (RMSE): 2.505202531814575\n"]}],"source":["model = Sequential()\n","model.add(Dense(64, input_dim=X.shape[1], activation='relu')) # Hidden 1\n","model.add(Dense(64,activation='relu')) #Hidden 2\n","model.add(Dense(1)) # Output\n","model.compile(loss='mean_squared_error', optimizer='adam')\n","model.fit(X_train,y_train,verbose=2,epochs=250)\n","\n","#With test data\n","pred = model.predict(X_test)\n","score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n","print(f\"Final score (RMSE): {score}\")"]},{"cell_type":"markdown","metadata":{"id":"RCBVOS6dRVUd"},"source":["The end result is improved RMSE score.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JCrN083TRVUe"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}